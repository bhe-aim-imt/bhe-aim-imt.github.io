<!DOCTYPE html>
<html lang="en" data-dark="false">
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <!--
  put your analytics (e.g. Google Analytics) tracking code here
-->

  <!--
  put your search engine verification (e.g. Google Search Console) tag here
-->

  


























<meta name="viewport" content="width=device-width, initial-scale=1">

<title>Research | BHE-AIM-IMT</title>

<link rel="icon" href="/images/icon.png">

<meta name="title" content="Research">
<meta name="description" content="Welcome to bhe-aim-imt website! Relying on The Key Laboratory of Brain Health Intelligent Evaluation and Intervention of the Ministry of Education, our lab is dedicated to solving the problem of intelligent audio intervention and contributing to the advancement of the discipline and society. We welcome people from all areas to visit our website to know more about our research achievements, team members and other contents. We look forward to working with you to explore the mysteries of science!">

<meta property="og:title" content="Research">
<meta property="og:site_title" content="BHE-AIM-IMT">
<meta property="og:description" content="Welcome to bhe-aim-imt website! Relying on The Key Laboratory of Brain Health Intelligent Evaluation and Intervention of the Ministry of Education, our lab is dedicated to solving the problem of intelligent audio intervention and contributing to the advancement of the discipline and society. We welcome people from all areas to visit our website to know more about our research achievements, team members and other contents. We look forward to working with you to explore the mysteries of science!">
<meta property="og:url" content="https://bhe-aim-imt.github.io">
<meta property="og:image" content="/images/share.jpg">
<meta property="og:locale" content="en_US">

<meta property="twitter:title" content="Research">
<meta property="twitter:description" content="Welcome to bhe-aim-imt website! Relying on The Key Laboratory of Brain Health Intelligent Evaluation and Intervention of the Ministry of Education, our lab is dedicated to solving the problem of intelligent audio intervention and contributing to the advancement of the discipline and society. We welcome people from all areas to visit our website to know more about our research achievements, team members and other contents. We look forward to working with you to explore the mysteries of science!">
<meta property="twitter:url" content="https://bhe-aim-imt.github.io">
<meta property="twitter:card" content="summary_large_image">
<meta property="twitter:image" content="/images/share.jpg">


  <meta property="og:type" content="website">


<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "WebSite",
    
    "name": "Research",
    "description": "Welcome to bhe-aim-imt website! Relying on The Key Laboratory of Brain Health Intelligent Evaluation and Intervention of the Ministry of Education, our lab is dedicated to solving the problem of intelligent audio intervention and contributing to the advancement of the discipline and society. We welcome people from all areas to visit our website to know more about our research achievements, team members and other contents. We look forward to working with you to explore the mysteries of science!",
    "headline": "Research",
    "publisher": {
      "@type": "Organization",
      "logo": { "@type": "ImageObject", "url": "/images/icon.png" }
    },
    "url": "https://bhe-aim-imt.github.io"
  }
</script>

<link rel="alternate" type="application/rss+xml" href="https://bhe-aim-imt.github.io/feed.xml">

  <!-- Google Fonts -->
<!-- automatically get url from fonts used in theme file -->

<link rel="preconnect" href="https://fonts.gstatic.com">
<link href="https://fonts.googleapis.com/css2?display=swap&&family=Barlow:ital,wght@0,200;0,400;0,500;0,600;1,200;1,400;1,500;1,600&amp;family=Roboto+Mono:ital,wght@0,200;0,400;0,500;0,600;1,200;1,400;1,500;1,600" rel="stylesheet">

<!-- Font Awesome icons (load asynchronously due to size) -->

<link href="https://use.fontawesome.com/releases/v6.5.0/css/all.css" rel="stylesheet" media="none" onload="this.removeAttribute('media'); this.onload = null;">
<noscript>
  <link href="https://use.fontawesome.com/releases/v6.5.0/css/all.css" rel="stylesheet">
</noscript>

  <!-- third party styles -->
<!-- https://stylishthemes.github.io/Syntax-Themes/pygments/ -->
<link href="https://cdn.jsdelivr.net/gh/StylishThemes/Syntax-Themes/pygments/css-github/pygments-tomorrow-night-eighties.css" rel="stylesheet">

<!-- include all sass in styles folder -->


  
    <link href="/_styles/-theme.css" rel="stylesheet">
  

  
    <link href="/_styles/alert.css" rel="stylesheet">
  

  
    <link href="/_styles/all.css" rel="stylesheet">
  

  
    <link href="/_styles/anchor.css" rel="stylesheet">
  

  
    <link href="/_styles/background.css" rel="stylesheet">
  

  
    <link href="/_styles/body.css" rel="stylesheet">
  

  
    <link href="/_styles/bold.css" rel="stylesheet">
  

  
    <link href="/_styles/button.css" rel="stylesheet">
  

  
    <link href="/_styles/card.css" rel="stylesheet">
  

  
    <link href="/_styles/checkbox.css" rel="stylesheet">
  

  
    <link href="/_styles/citation.css" rel="stylesheet">
  

  
    <link href="/_styles/code.css" rel="stylesheet">
  

  
    <link href="/_styles/cols.css" rel="stylesheet">
  

  
    <link href="/_styles/dark-toggle.css" rel="stylesheet">
  

  
    <link href="/_styles/details.css" rel="stylesheet">
  

  
    <link href="/_styles/feature.css" rel="stylesheet">
  

  
    <link href="/_styles/figure.css" rel="stylesheet">
  

  
    <link href="/_styles/float.css" rel="stylesheet">
  

  
    <link href="/_styles/font.css" rel="stylesheet">
  

  
    <link href="/_styles/footer.css" rel="stylesheet">
  

  
    <link href="/_styles/form.css" rel="stylesheet">
  

  
    <link href="/_styles/grid.css" rel="stylesheet">
  

  
    <link href="/_styles/header.css" rel="stylesheet">
  

  
    <link href="/_styles/heading.css" rel="stylesheet">
  

  
    <link href="/_styles/highlight.css" rel="stylesheet">
  

  
    <link href="/_styles/icon.css" rel="stylesheet">
  

  
    <link href="/_styles/image.css" rel="stylesheet">
  

  
    <link href="/_styles/link.css" rel="stylesheet">
  

  
    <link href="/_styles/list.css" rel="stylesheet">
  

  
    <link href="/_styles/main.css" rel="stylesheet">
  

  
    <link href="/_styles/paragraph.css" rel="stylesheet">
  

  
    <link href="/_styles/portrait.css" rel="stylesheet">
  

  
    <link href="/_styles/post-excerpt.css" rel="stylesheet">
  

  
    <link href="/_styles/post-info.css" rel="stylesheet">
  

  
    <link href="/_styles/post-nav.css" rel="stylesheet">
  

  
    <link href="/_styles/quote.css" rel="stylesheet">
  

  
    <link href="/_styles/rule.css" rel="stylesheet">
  

  
    <link href="/_styles/search-box.css" rel="stylesheet">
  

  
    <link href="/_styles/search-info.css" rel="stylesheet">
  

  
    <link href="/_styles/section.css" rel="stylesheet">
  

  
    <link href="/_styles/table.css" rel="stylesheet">
  

  
    <link href="/_styles/tags.css" rel="stylesheet">
  

  
    <link href="/_styles/textbox.css" rel="stylesheet">
  

  
    <link href="/_styles/tooltip.css" rel="stylesheet">
  

  
    <link href="/_styles/util.css" rel="stylesheet">
  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  


<!-- include all css in styles folder -->



  <!-- third party scripts -->
<script src="https://unpkg.com/@popperjs/core@2" defer></script>
<script src="https://unpkg.com/tippy.js@6" defer></script>
<script src="https://unpkg.com/mark.js@8" defer></script>

<!-- include all js in scripts folder -->


  <script src="/_scripts/anchors.js"></script>

  <script src="/_scripts/dark-mode.js"></script>

  <script src="/_scripts/fetch-tags.js"></script>

  <script src="/_scripts/search.js"></script>

  <script src="/_scripts/site-search.js"></script>

  <script src="/_scripts/table-wrap.js"></script>

  <script src="/_scripts/tooltip.js"></script>


</head>

  <body>
    







<header class="background" style="--image: url('/images/background_brain.jpg')" data-dark="true">
  <a href="/" class="home">
    
      <span class="logo">
        
          <svg xmlns="http://www.w3.org/2000/svg" viewbox="-40 -60 80 100">
  <style>
    .bubble {
      animation: float 2s ease-out both infinite var(--delay);
    }
    @keyframes float {
      0% {
        opacity: 0;
      }
      50% {
        transform: translateY(0);
        opacity: 0;
      }
      75% {
        opacity: 1;
      }
      100% {
        opacity: 0;
        transform: translateY(-40px);
      }
    }
  </style>
  <g fill="currentColor" opacity="0.5">
    <circle class="bubble" cx="0" cy="-10" r="3" style="--delay: 0.1s"></circle>
    <circle class="bubble" cx="0" cy="-10" r="3" style="--delay: 0.4s"></circle>
    <circle class="bubble" cx="0" cy="-10" r="3" style="--delay: 1.1s"></circle>
  </g>
  <path fill="#38bdf8" d="
      M 0 -22.5
      L -19.5 -11.25
      L -19.5 11.25
      L 0 22.5
      L 19.5 11.25
      L 19.5 -11.25
      z
    "></path>
  <path fill="#bae6fd" d="
      M 0 -22.5
      L -19.5 -11.25
      L 0 0
      L 19.5 -11.25
      z
    "></path>
  <path fill="none" stroke="currentColor" stroke-width="5" d="
      M -18 -53
      L -10 -53
      L -10 -29.2
      L -30.3 -17.5
      L -30.3 17.5
      L 0 35
      L 30.3 17.5
      L 30.3 -17.5
      L 10 -29.2
      L 10 -53
      L 18 -53
    "></path>
</svg>

        
      </span>
    
    
      <span class="title-text" data-tooltip="Home">
        
          <span class="title">BHE-AIM-IMT</span>
        
        
      </span>
    
  </a>

  <input class="nav-toggle" type="checkbox" aria-label="show/hide nav">

  <nav>
    
    
      
        <a href="/team/" data-tooltip="About our team">
          Team
        </a>
      
    
      
        <a href="/research/" data-tooltip="Published works">
          Research
        </a>
      
    
      
        <a href="/projects/" data-tooltip="Projects">
          Projects
        </a>
      
    
      
        <a href="/thesis/" data-tooltip="In-Progress Thesis and Completed Thesis">
          Thesis
        </a>
      
    
      
        <a href="/devices/" data-tooltip="Devices">
          Devices
        </a>
      
    
      
        <a href="/awards/" data-tooltip="Awards">
          Awards
        </a>
      
    
  </nav>
</header>

    <main>
      <!--
  modify main content of page:
  - add section breaks
  - attach section properties
  - filter out blank sections
-->






  
  
  

  <section class="background" data-size="page">
    <!--
  background: ;
  dark: ;
  size: ;
-->

<h2 id="all">All</h2>

<div class="search-box">
  <input type="text" class="search-input" oninput="onSearchInput(this)" placeholder="Search items on this page">
  <button disabled data-tooltip="Clear search" aria-label="clear search" onclick="onSearchClear()">
    <i class="icon fa-solid fa-magnifying-glass"></i>
  </button>
</div>

<div class="search-info"></div>

<h3 id="2025">2025</h3>

<div class="citation-container">
  <div class="citation">
    
      <a href="https://doi.org/g864n5" class="citation-image" aria-label="An AI-Assisted All-in-One Integrated Coronary Artery Disease Diagnosis System Using a Portable Heart Sound Sensor With an On-Board Executable Lightweight Model">
        <img src="/images/TMC_2025.jpg" alt="An AI-Assisted All-in-One Integrated Coronary Artery Disease Diagnosis System Using a Portable Heart Sound Sensor With an On-Board Executable Lightweight Model" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
      </a>
    

    <div class="citation-text">
      
      

      <a href="https://doi.org/g864n5" class="citation-title">
        An AI-Assisted All-in-One Integrated Coronary Artery Disease Diagnosis System Using a Portable Heart Sound Sensor With an On-Board Executable Lightweight Model
      </a>

      <div class="citation-authors" data-tooltip="Haojie Zhang, Fuze Tian, Yang Tan, Lin Shen, Jingyu Liu, Jie Liu, Kun Qian, Yalei Han, Gong Su, Bin Hu, Björn W. Schuller, Yoshiharu Yamamoto" tabindex="0">
        Haojie Zhang, Fuze Tian, Yang Tan, Lin Shen, Jingyu Liu, …, Yalei Han, Gong Su, Bin Hu, Björn W. Schuller, Yoshiharu Yamamoto

      </div>

      <div class="citation-details">
        <span class="citation-publisher">IEEE Transactions on Mobile Computing</span>
         · 
        <span class="citation-date">01 Aug 2025</span>
         · 
        <span class="citation-id">doi:10.1109/TMC.2025.3547842</span>
      </div>

      
        
          <div class="citation-description">
            Heart sounds play a crucial role in assessing Coronary Artery Disease (CAD). The advancement of Artificial Intelligence (AI) technologies has given rise to Computer Audition (CA)-based methods for CAD detection. However, previous research has focused primarily on analyzing and modeling heart sound data, overlooking practical application scenarios. In this work, we design a pervasive heart sound collection device used for high-quality heart sound data acquisition. Moreover, we introduce an on-board executable lightweight network tailored for the designed portable device, referred to as TYKDModel. Further, heart sound data from 41 CAD patients and 22 non-CAD healthy controls are collected using the developed device. Experimental results show that the TYKDModel exhibits low-computational complexity, with 52.16 K parameters and 5.03 M Floating-Point Operations (FLOPs). When deployed on the board, it requires only 1.10 MB of Random Access Memory (RAM) and 236.27 KB of Read-Only Memory (ROM), and takes around 1.72 seconds to perform a classification. Despite the low computational and spatial complexity, the TYKDModel achieves a notable classification accuracy of 85.2%, specificity of 88.6%, and sensitivity of 82.8% on the board. These results indicate the promising potential of AI-assisted all-in-one integrated system for the diagnosis of heart sound-assisted CAD.

          </div>
        

        

        
      
    </div>
  </div>
</div>

<div class="citation-container">
  <div class="citation">
    
      <a href="https://doi.org/g9hvdz" class="citation-image" aria-label="MDH-NAS: Accelerating EEG Signal Classification With Mixed-Level Differentiable and Hardware-Aware Neural Architecture Search">
        <img src="/images/IoT_2025.png" alt="MDH-NAS: Accelerating EEG Signal Classification With Mixed-Level Differentiable and Hardware-Aware Neural Architecture Search" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
      </a>
    

    <div class="citation-text">
      
      

      <a href="https://doi.org/g9hvdz" class="citation-title">
        MDH-NAS: Accelerating EEG Signal Classification With Mixed-Level Differentiable and Hardware-Aware Neural Architecture Search
      </a>

      <div class="citation-authors" tabindex="0">
        Lixian Zhu, Su Wang, Xiaokun Jin, Kai Zheng, Jian Zhang, Shuting Sun, Fuze Tian, Ran Cai, Bin Hu

      </div>

      <div class="citation-details">
        <span class="citation-publisher">IEEE Internet of Things Journal</span>
         · 
        <span class="citation-date">01 Jul 2025</span>
         · 
        <span class="citation-id">doi:10.1109/JIOT.2025.3553450</span>
      </div>

      
        
          <div class="citation-description">
            In noninvasive brain-computer interfaces (BCIs), EEG analysis plays a critical role, with neural networks serving as a cornerstone for signal decoding. Existing neural network approaches for EEG signal recognition require extensive manual design and hyperparameter tuning, leading to inefficiencies and making them impractical for embedded devices due to their large model size. To address these limitations, we propose mixed-level differentiable and hardware-aware neural architecture search (MDH-NAS), a framework that automatically generates lightweight neural networks tailored for EEG classification. Unlike traditional DARTS methods, MDH-NAS employs a hybrid optimization strategy that balances global and local search spaces, thereby accelerating and refining architecture discovery. It introduces explicit size constraints during the search process to ensure deployability on embedded devices. MDH-NAS demonstrates autonomous generation of architectures for tasks such as motor imagery (MI) and depression recognition, achieving 87.80% accuracy on the BCI-IV dataset and 90.09% on the MODMA dataset. When deployed on the EAIDK-610 board across heterogeneous tasks, it attains 85.37% accuracy on the EEG Motor Movement/Imagery dataset. This method reduces architecture discovery time by 89% and enhances prediction accuracy by 8.70% compared to baseline methods, highlighting its potential for scalable EEG analysis and real-world embedded deployment.

          </div>
        

        

        
      
    </div>
  </div>
</div>

<div class="citation-container">
  <div class="citation">
    
      <a href="https://doi.org/g9tt34" class="citation-image" aria-label="Machine Learning Enabled Reusable Adhesion, Entangled Network-Based Hydrogel for Long-Term, High-Fidelity EEG Recording and Attention Assessment">
        <img src="/images/nano_2025.png" alt="Machine Learning Enabled Reusable Adhesion, Entangled Network-Based Hydrogel for Long-Term, High-Fidelity EEG Recording and Attention Assessment" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
      </a>
    

    <div class="citation-text">
      
      

      <a href="https://doi.org/g9tt34" class="citation-title">
        Machine Learning Enabled Reusable Adhesion, Entangled Network-Based Hydrogel for Long-Term, High-Fidelity EEG Recording and Attention Assessment
      </a>

      <div class="citation-authors" data-tooltip="Kai Zheng, Chengcheng Zheng, Lixian Zhu, Bihai Yang, Xiaokun Jin, Su Wang, Zikai Song, Jingyu Liu, Yan Xiong, Fuze Tian, Ran Cai, Bin Hu" tabindex="0">
        Kai Zheng, Chengcheng Zheng, Lixian Zhu, Bihai Yang, Xiaokun Jin, …, Jingyu Liu, Yan Xiong, Fuze Tian, Ran Cai, Bin Hu

      </div>

      <div class="citation-details">
        <span class="citation-publisher">Nano-Micro Letters</span>
         · 
        <span class="citation-date">29 May 2025</span>
         · 
        <span class="citation-id">doi:10.1007/s40820-025-01780-7</span>
      </div>

      
        
          <div class="citation-description">
            Due to their high mechanical compliance and excellent biocompatibility, conductive hydrogels exhibit significant potential for applications in flexible electronics. However, as the demand for high sensitivity, superior mechanical properties, and strong adhesion performance continues to grow, many conventional fabrication methods remain complex and costly. Herein, we propose a simple and efficient strategy to construct an entangled network hydrogel through a liquid–metal-induced cross-linking reaction, hydrogel demonstrates outstanding properties, including exceptional stretchability (1643%), high tensile strength (366.54 kPa), toughness (350.2 kJ m−3), and relatively low mechanical hysteresis. The hydrogel exhibits long-term stable reusable adhesion (104 kPa), enabling conformal and stable adhesion to human skin. This capability allows it to effectively capture high-quality epidermal electrophysiological signals with high signal-to-noise ratio (25.2 dB) and low impedance (310 ohms). Furthermore, by integrating advanced machine learning algorithms, achieving an attention classification accuracy of 91.38%, which will significantly impact fields like education, healthcare, and artificial intelligence.

          </div>
        

        

        
      
    </div>
  </div>
</div>

<div class="citation-container">
  <div class="citation">
    
      <a href="https://doi.org/g9hvd2" class="citation-image" aria-label="Explainable Depression Classification Based on EEG Feature Selection From Audio Stimuli">
        <img src="/images/TNSRE_2025.jpg" alt="Explainable Depression Classification Based on EEG Feature Selection From Audio Stimuli" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
      </a>
    

    <div class="citation-text">
      
      

      <a href="https://doi.org/g9hvd2" class="citation-title">
        Explainable Depression Classification Based on EEG Feature Selection From Audio Stimuli
      </a>

      <div class="citation-authors" data-tooltip="Lixian Zhu, Rui Wang, Xiaokun Jin, Yuwen Li, Fuze Tian, Ran Cai, Kun Qian, Xiping Hu, Bin Hu, Yoshiharu Yamamoto, Björn W. Schuller" tabindex="0">
        Lixian Zhu, Rui Wang, Xiaokun Jin, Yuwen Li, Fuze Tian, …, Kun Qian, Xiping Hu, Bin Hu, Yoshiharu Yamamoto, Björn W. Schuller

      </div>

      <div class="citation-details">
        <span class="citation-publisher">IEEE Transactions on Neural Systems and Rehabilitation Engineering</span>
         · 
        <span class="citation-date">01 Jan 2025</span>
         · 
        <span class="citation-id">doi:10.1109/TNSRE.2025.3557275</span>
      </div>

      
        
          <div class="citation-description">
            With the development of affective computing and Artificial Intelligence (AI) technologies, Electroencephalogram (EEG)-based depression detection methods have been widely proposed. However, existing studies have mostly focused on the accuracy of depression recognition, ignoring the association between features and models. Additionally, there is a lack of research on the contribution of different features to depression recognition. To this end, this study introduces an innovative approach to depression detection using EEG data, integrating Ant-Lion Optimization (ALO) and Multi-Agent Reinforcement Learning (MARL) for feature fusion analysis. The inclusion of Explainable Artificial Intelligence (XAI) methods enhances the explainability of the model’s features. The Time-Delay Embedded Hidden Markov Model (TDE-HMM) is employed to infer internal brain states during depression, triggered by audio stimulation. The ALO-MARL algorithm, combined with hyper-parameter optimization of the XGBoost classifier, achieves high accuracy (93.69%), sensitivity (88.60%), specificity (97.08%), and F1-score (91.82%) on a auditory stimulus-evoked three-channel EEG dataset. The results suggest that this approach outperforms state-of-the-art feature selection methods for depression recognition on this dataset, and XAI elucidates the critical impact of the minimum value of Power Spectral Density (PSD), Sample Entropy (SampEn), and Rényi Entropy (Ren) on depression recognition. The study also explores dynamic brain state transitions revealed by audio stimuli, providing insights for the clinical application of AI algorithms in depression recognition.

          </div>
        

        

        
      
    </div>
  </div>
</div>

<div class="citation-container">
  <div class="citation">
    
      <a href="https://doi.org/g9tt3t" class="citation-image" aria-label="Exploring the Alleviating Effects of taVNS on Negative Emotions: An EEG Study">
        <img src="" alt="Exploring the Alleviating Effects of taVNS on Negative Emotions: An EEG Study" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
      </a>
    

    <div class="citation-text">
      
      

      <a href="https://doi.org/g9tt3t" class="citation-title">
        Exploring the Alleviating Effects of taVNS on Negative Emotions: An EEG Study
      </a>

      <div class="citation-authors" tabindex="0">
        Xiaokun Jin, Chengcheng Zheng, Mingyue Jin, Qunxi Dong, Lixian Zhu, Fuze Tian

      </div>

      <div class="citation-details">
        <span class="citation-publisher">IEEE Transactions on Computational Social Systems</span>
         · 
        <span class="citation-date">01 Jan 2025</span>
         · 
        <span class="citation-id">doi:10.1109/TCSS.2025.3564035</span>
      </div>

      
        
          <div class="citation-description">
            Emotion inhibitory control is a key executive function of the human brain, which regulates behavior by suppressing inappropriate responses. It plays an integral part in alleviating negative emotions, improving mood, and preventing depression. Transcutaneous auricular vagus nerve stimulation (taVNS) has been proved to enhance behavioral control, potentially suppressing negative emotions or facilitating their reduction in healthy individuals. However, the neurocomputational mechanisms underlying taVNS-induced neuroenhancement remain unclear. In this work, a portable electroencephalography (EEG) acquisition and stimulation device is designed to collect eight-channel EEG signals and deliver taVNS to both sides of ears. Then, we design a protocol that successfully induced negative emotions in healthy subjects. Next, we conduct a sham-controlled experiment, involving 28 healthy subjects, to explore the changes in EEG of negative emotions under taVNS. Finally, we primarily analyze the power spectrum density (PSD) of EEG signals and the functional connectivity network of the brain, based on the phase locking value (PLV), to assess the effect of taVNS on neural activity induced by negative emotions. The results of the experiment reveal that taVNS is a promising method for enhancing emotional inhibitory control by reducing PSD in the alpha band and enhancing PLV within prefrontal inhibitory control networks. In addition, differences in graph theory parameters between the Sham and taVNS conditions indicate that taVNS helps regulate negative emotions. In conclusion, this study demonstrates that taVNS enhances inhibitory control and reveals its neurocomputational mechanisms of EEG in healthy individuals during the development of negative emotions. And results indicate that taVNS could serve as a promising neuromodulation therapy for psychiatric disorders and individuals with depression or emotional distress.

          </div>
        

        

        
      
    </div>
  </div>
</div>

<div class="citation-container">
  <div class="citation">
    
      <a href="https://doi.org/g9dk2w" class="citation-image" aria-label="Enhancing Emotion Regulation in Mental Disorder Treatment: An AIGC-based Closed-Loop Music Intervention System">
        <img src="/images/TAFFC_2025.jpg" alt="Enhancing Emotion Regulation in Mental Disorder Treatment: An AIGC-based Closed-Loop Music Intervention System" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
      </a>
    

    <div class="citation-text">
      
      

      <a href="https://doi.org/g9dk2w" class="citation-title">
        Enhancing Emotion Regulation in Mental Disorder Treatment: An AIGC-based Closed-Loop Music Intervention System
      </a>

      <div class="citation-authors" tabindex="0">
        Lin Shen, Haojie Zhang, Cuiping Zhu, Ruobing Li, Kun Qian, Fuze Tian, Bin Hu, Björn W. Schuller, Yoshiharu Yamamoto

      </div>

      <div class="citation-details">
        <span class="citation-publisher">IEEE Transactions on Affective Computing</span>
         · 
        <span class="citation-date">01 Jan 2025</span>
         · 
        <span class="citation-id">doi:10.1109/TAFFC.2025.3557873</span>
      </div>

      
        
          <div class="citation-description">
            Mental disorders have increased rapidly and have emerged as a serious social health issue in the recent decade. Undoubtedly, the timely treatment of mental disorders is crucial. Emotion regulation has been proven to be an effective method for treating mental disorders. Music therapy as one of the methods that can achieve emotional regulation has gained increasing attention in the field of mental disorder treatment. However, traditional music therapy methods still face some unresolved issues, such as the lack of real-time capability and the inability to form closed-loop systems. With the advancement of artificial intelligence (AI), especially AI-generated content (AIGC), AI-based music therapy holds promise in addressing these issues. In this paper, an AIGC-based closed-loop music intervention system demonstration is proposed to regulate emotions for mental disorder treatment. This system demonstration consists of an emotion recognition model and a music generation model. The emotion recognition model can assess mental states, while the music generation model generates the corresponding emotional music for regulation. The system continuously performs recognition and regulation, thus forming a closed-loop process. In the experiment, we first conduct experiments on both the emotion recognition model and the music generation model to validate the accuracy of the recognition model and the music quality generated by the music generation models. In conclusion, we conducted comprehensive tests on the entire system to verify its feasibility and effectiveness.

          </div>
        

        

        
      
    </div>
  </div>
</div>

<div class="citation-container">
  <div class="citation">
    
      <a href="https://doi.org/g86jh9" class="citation-image" aria-label="An On-Board Executable Multi-Feature Transfer-Enhanced Fusion Model for Three-Lead EEG Sensor-Assisted Depression Diagnosis">
        <img src="/images/JBHI_2024.jpg" alt="An On-Board Executable Multi-Feature Transfer-Enhanced Fusion Model for Three-Lead EEG Sensor-Assisted Depression Diagnosis" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
      </a>
    

    <div class="citation-text">
      
      

      <a href="https://doi.org/g86jh9" class="citation-title">
        An On-Board Executable Multi-Feature Transfer-Enhanced Fusion Model for Three-Lead EEG Sensor-Assisted Depression Diagnosis
      </a>

      <div class="citation-authors" tabindex="0">
        Fuze Tian, Haojie Zhang, Yang Tan, Lixian Zhu, Lin Shen, Kun Qian, Bin Hu, Björn W. Schuller, Yoshiharu Yamamoto

      </div>

      <div class="citation-details">
        <span class="citation-publisher">IEEE Journal of Biomedical and Health Informatics</span>
         · 
        <span class="citation-date">01 Jan 2025</span>
         · 
        <span class="citation-id">doi:10.1109/JBHI.2024.3487012</span>
      </div>

      
        
          <div class="citation-description">
            The development of affective computing and medical electronic technologies has led to the emergence of Artificial Intelligence (AI)-based methods for the early detection of depression. 
However, previous studies have often overlooked the necessity for the AI-assisted diagnosis system to be wearable and accessible in practical scenarios for depression recognition. 
In this work, we present an on-board executable multi-feature transfer-enhanced fusion model for our custom-designed wearable three-lead Electroencephalogram (EEG) sensor, based on EEG data collected from 73 depressed patients and 108 healthy controls. 
Experimental results show that the proposed model exhibits low-computational complexity (65.0 K parameters), promising Floating-Point Operations (FLOPs) performance (25.6 M), real-time processing (1.5 s/execution), and low power consumption (320.8 mW). 
Furthermore, it requires only 202.0 KB of Random Access Memory (RAM) and 279.6 KB of Read-Only Memory (ROM) when deployed on the EEG sensor. 
Despite its low computational and spatial complexity, the model achieves a notable classification accuracy of 95.2%, specificity of 94.0%, and sensitivity of 96.9% under independent test conditions. 
These results underscore the potential of deploying the model on the wearable three-lead EEG sensor for assisting in the diagnosis of depression.

          </div>
        

        

        
      
    </div>
  </div>
</div>

<h3 id="2024">2024</h3>

<div class="citation-container">
  <div class="citation">
    
      <a href="https://doi.org/g87m3f" class="citation-image" aria-label="Advancements in Affective Disorder Detection: Using Multimodal Physiological Signals and Neuromorphic Computing Based on SNNs">
        <img src="/images/tian_3.jpg" alt="Advancements in Affective Disorder Detection: Using Multimodal Physiological Signals and Neuromorphic Computing Based on SNNs" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
      </a>
    

    <div class="citation-text">
      
      

      <a href="https://doi.org/g87m3f" class="citation-title">
        Advancements in Affective Disorder Detection: Using Multimodal Physiological Signals and Neuromorphic Computing Based on SNNs
      </a>

      <div class="citation-authors" tabindex="0">
        Fuze Tian, Lixin Zhang, Lixian Zhu, Mingqi Zhao, Jingyu Liu, Qunxi Dong, Qinglin Zhao

      </div>

      <div class="citation-details">
        <span class="citation-publisher">IEEE Transactions on Computational Social Systems</span>
         · 
        <span class="citation-date">01 Dec 2024</span>
         · 
        <span class="citation-id">doi:10.1109/TCSS.2024.3420445</span>
      </div>

      
        
          <div class="citation-description">
            Currently, the integration of artificial intelligence (AI) techniques with multimodal physiological signals represents a pivotal approach to detect affective disorders (ADs). With the increasing complexity and diversity of physiological signal modalities, researchers have introduced various AI methods using multimodal physiological signals to improve model classification performance and explainability to increase trust and facilitate clinical adoption. Among these methods, spiking neural networks (SNNs) stand out as a promising avenue due to their alignment with the operating principles of the human brain, robust biological explainability, and adeptness in processing spatial–temporal information in an efficient event-driven manner with low power consumption. Furthermore, the emergence of neuromorphic computing (NC) chips based on SNNs has greatly bolstered the field of NC, enabling effective support for objective, pervasive, and wearable AI-assisted medical diagnostic devices for ADs and other diseases. This article presents a review of recent achievements in multimodal AD detection and points out the associated challenges in utilizing multimodal physiological signals and NC based on SNNs for AD detection. Building upon this foundation, we give perspectives on future work. The intended readership for this review consists of researchers in the fields of cognitive computing, computational psychophysiology, affective computing, NC, and brain-inspired computing. We hope that this survey not only garners increased attention from the scientific community but also serves as a valuable guide for future studies in this field.

          </div>
        

        

        
      
    </div>
  </div>
</div>

<div class="citation-container">
  <div class="citation">
    
      <a href="https://doi.org/g86jjb" class="citation-image" aria-label="A First Look at Generative Artificial Intelligence Based Music Therapy for Mental Disorders">
        <img src="/images/TCE_2024.jpg" alt="A First Look at Generative Artificial Intelligence Based Music Therapy for Mental Disorders" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
      </a>
    

    <div class="citation-text">
      
      

      <a href="https://doi.org/g86jjb" class="citation-title">
        A First Look at Generative Artificial Intelligence Based Music Therapy for Mental Disorders
      </a>

      <div class="citation-authors" tabindex="0">
        Lin Shen, Haojie Zhang, Cuiping Zhu, Ruobing Li, Kun Qian, Wei Meng, Fuze Tian, Bin Hu, Björn W. Schuller, Yoshiharu Yamamoto

      </div>

      <div class="citation-details">
        <span class="citation-publisher">IEEE Transactions on Consumer Electronics</span>
         · 
        <span class="citation-date">01 Dec 2024</span>
         · 
        <span class="citation-id">doi:10.1109/TCE.2024.3514633</span>
      </div>

      
        
          <div class="citation-description">
            Mental disorders show a rapid increase and cause considerable harm to individuals as well as the society in recent decade. 
Hence, mental disorders have become a serious public health challenge in nowadays society. 
Timely treatment of mental disorders plays a critical role for reducing the harm of mental illness to individuals and society. 
Music therapy is a type of non-pharmaceutical method in treating such mental disorders. 
However, conventional music therapy suffers from a number of issues resulting in a lack of popularity. 
Thanks to the rapid development of Artificial Intelligence (AI), especially the AI Generated Content (AIGC), it provides a chance to address these issues. Nevertheless, to the best of our knowledge, there is no work investigating music therapy from AIGC and closed-loop perspective. 
In this paper, we summarise some universal music therapy methods and discuss their shortages. Then, we indicate some AIGC techniques, especially the music generation, for their application in music therapy. 
Moreover, we present a closed-loop music therapy system and introduce its implementation details. 
Finally, we discuss some challenges in AIGC-based music therapy with proposing further research direction, and we suggest the potential of this system to become a consumer-grade product for treating mental disorders.

          </div>
        

        

        
      
    </div>
  </div>
</div>

<div class="citation-container">
  <div class="citation">
    
      <a href="https://doi.org/g9hvd3" class="citation-image" aria-label="Design and Implementation of Electroacupuncture: A Study of Prefrontal EEG Characteristics Under taVNS">
        <img src="/images/JSEN_2024.jpg" alt="Design and Implementation of Electroacupuncture: A Study of Prefrontal EEG Characteristics Under taVNS" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
      </a>
    

    <div class="citation-text">
      
      

      <a href="https://doi.org/g9hvd3" class="citation-title">
        Design and Implementation of Electroacupuncture: A Study of Prefrontal EEG Characteristics Under taVNS
      </a>

      <div class="citation-authors" tabindex="0">
        Lixian Zhu, Yanan Zhao, Xiaokun Jin, Fuze Tian, Jingxin Liu, Ran Cai, Qunxi Dong, Peijing Rong, Bin Hu

      </div>

      <div class="citation-details">
        <span class="citation-publisher">IEEE Sensors Journal</span>
         · 
        <span class="citation-date">15 Oct 2024</span>
         · 
        <span class="citation-id">doi:10.1109/JSEN.2024.3441619</span>
      </div>

      
        
          <div class="citation-description">
            Transcutaneous auricular vagus nerve stimulation (taVNS), as a method for mimicking VNS, has been proven effective in the treatment of psychiatric disorders. However, the underlying mechanism through which taVNS mimics VNS remains elusive. Moreover, the parameters of taVNS are singularly fixed and open loop in previous work, which is difficult to apply to all users as individual differences are inevitable. Since electroencephalogram (EEG) is one of the important biomarkers of neural activity, this study aims to develop a closed-loop system for personalized interventions in emotion regulation by integrating taVNS with EEG feedback. We first design a taVNS system based on EEG signal feedback and verify the performance metrics of the system. Second, we design experimental paradigms to explore the changes in EEG features under the taVNS. The experimental results show that the EEG characteristics differ between different taVNS frequencies (between 50 and 100 Hz). Moreover, we observe substantial distinctions between EEG characteristics during the taVNS state and the resting state, with pre-taVNS, taVNS, and post-taVNS exhibiting notable differences. Specifically, the power spectral density (PSD) in the taVNS state is lower than in the resting state ( p&lt;0.05 ), except for the beta band where the opposite trend is observed. Additionally, features such as Lempel-Ziv complexity (LZC) and Reyi entropy (REn) displayed a decreasing trend throughout the taVNS ( p&lt;0.05 ). Furthermore, we employ hidden Markov models (HMMs) to reveal the heterogeneity of dynamic changes in the brain during taVNS, providing a mechanistic interpretation of taVNS.

          </div>
        

        

        
      
    </div>
  </div>
</div>

<div class="citation-container">
  <div class="citation">
    
      <a href="https://doi.org/g86jjf" class="citation-image" aria-label="E-ODN: An Emotion Open Deep Network for Generalised and Adaptive Speech Emotion Recognition">
        <img src="/images/ISP_2024.jpg" alt="E-ODN: An Emotion Open Deep Network for Generalised and Adaptive Speech Emotion Recognition" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
      </a>
    

    <div class="citation-text">
      
      

      <a href="https://doi.org/g86jjf" class="citation-title">
        E-ODN: An Emotion Open Deep Network for Generalised and Adaptive Speech Emotion Recognition
      </a>

      <div class="citation-authors" tabindex="0">
        Liuxian Ma, Lin Shen, Ruobing Li, Haojie Zhang, Kun Qian, Bin Hu, Björn W. Schuller, Yoshiharu Yamamoto

      </div>

      <div class="citation-details">
        <span class="citation-publisher">Interspeech 2024</span>
         · 
        <span class="citation-date">01 Sep 2024</span>
         · 
        <span class="citation-id">doi:10.21437/Interspeech.2024-685</span>
      </div>

      
        
          <div class="citation-description">
            Recognising the widest range of emotions possible is a major challenge in the task of Speech Emotion Recognition(SER), especially for complex and mixed emotions. 
However, due to the limited number of emotional types and uneven distribution of data within existing datasets, current SER models are typically trained and used in a narrow range of emotional types. 
In this paper, we propose the Emotion Open Deep Network(E-ODN) model to address this issue. 
Besides, we introduce a novel Open-Set Recognition method that maps sample emotional features into a three-dimensional emotional space. 
The method can infer unknown emotions and initialise new type weights, enabling the model to dynamically learn and infer emerging emotional types. 
The empirical results show that our recognition model outperforms the state-of-the-art(SOTA) models in dealing with multi-type unbalanced data, and it can also perform finer-grained emotion recognition.

          </div>
        

        

        
      
    </div>
  </div>
</div>

<div class="citation-container">
  <div class="citation">
    
      <a href="https://doi.org/g87m3c" class="citation-image" aria-label="An FFT-Based DC Offset Compensation and I Q Imbalance Correction Algorithm for Bioradar Sensors">
        <img src="/images/tian_1.jpg" alt="An FFT-Based DC Offset Compensation and I Q Imbalance Correction Algorithm for Bioradar Sensors" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
      </a>
    

    <div class="citation-text">
      
      

      <a href="https://doi.org/g87m3c" class="citation-title">
        An FFT-Based DC Offset Compensation and I/Q Imbalance Correction Algorithm for Bioradar Sensors
      </a>

      <div class="citation-authors" tabindex="0">
        Fuze Tian, Lixian Zhu, Qiuxia Shi, Xiaokun Jin, Ran Cai, Qunxi Dong, Qinglin Zhao, Bin Hu

      </div>

      <div class="citation-details">
        <span class="citation-publisher">IEEE Transactions on Microwave Theory and Techniques</span>
         · 
        <span class="citation-date">01 Mar 2024</span>
         · 
        <span class="citation-id">doi:10.1109/TMTT.2023.3308190</span>
      </div>

      
        
          <div class="citation-description">
            The challenge of noncontact presentation of human cardiopulmonary activity using a bioradar sensor is to linearly demodulate the Doppler cardiopulmonary diagram (DCD) signal from baseband signals. Arctangent demodulation can perform linear phase demodulation to obtain the DCD signal. However, the high-order harmonics and intermodulation terms (ITs) caused by the time-varying direct current (dc) offset and in-phase and quadrature-phase (I/Q) imbalance in the baseband signals significantly degrade the signal-to-noise ratio (SNR) of the Doppler heartbeat diagram (DHD) signal. In this work, a fast Fourier transform (FFT)-based algorithm is proposed to simultaneously perform time-varying dc offset compensation and I/Q imbalance correction without the need for an auxiliary device to improve the accuracy of the arctangent demodulation. The obtained results show that the SNRs of the algorithm-processed DHD signals are increased from 30.08 ± 2.41 to 68.88 ± 10.57 dB. In addition, the root mean square errors (RMSEs) of the C-C intervals of the DHD signals for eight subjects with respect to the J-J intervals of the ballistocardiogram (BCG) signals are 17.79 ± 2.72 ms (2.80% ± 0.43%), suggesting a promising potential of the DHD signal for noncontact biomedical applications.

          </div>
        

        

        
      
    </div>
  </div>
</div>

<h3 id="2023">2023</h3>

<div class="citation-container">
  <div class="citation">
    
      <a href="https://doi.org/g87m3h" class="citation-image" aria-label="Design and Verification of an Aromatherapy Feedback System for Mental Fatigue Based on Physiological Signals">
        <img src="/images/tian_5.jpg" alt="Design and Verification of an Aromatherapy Feedback System for Mental Fatigue Based on Physiological Signals" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
      </a>
    

    <div class="citation-text">
      
      

      <a href="https://doi.org/g87m3h" class="citation-title">
        Design and Verification of an Aromatherapy Feedback System for Mental Fatigue Based on Physiological Signals
      </a>

      <div class="citation-authors" tabindex="0">
        Tao Sun, Fuze Tian, Hua Jiang, Qinglin Zhao, Bin Hu

      </div>

      <div class="citation-details">
        <span class="citation-publisher">2023 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)</span>
         · 
        <span class="citation-date">05 Dec 2023</span>
         · 
        <span class="citation-id">doi:10.1109/BIBM58861.2023.10385577</span>
      </div>

      
        
          <div class="citation-description">
            Mental fatigue is a prevalent issue in contemporary society and can negatively affect physical performance and concentration, increasing the likelihood of adverse consequences due to inattention during productive activities. Therefore, it becomes increasingly important to address and eliminate fatigue within a specific period of time. Aromatherapy, as a form of Complementary Alternative Medicine (CAM), is a non-invasive, cost-effective, and efficient method to combat fatigue. Previous studies have assessed the effects of specific aromatherapy oils using scales, but there is a lack of objective and reliable physiological indicators to prove the effectiveness of aromatherapy. Hence, this paper seeks to establish a model illustrating the effects of aromatic essential oil gases on the human body. A multimodal physiological fatigue signal acquisition system that integrates aromatherapy feedback was designed. In addition, an experimental paradigm was developed to explore the potential of aromatherapy in mitigating mental fatigue. Electroencephalogram (EEG) and Electrocardiogram (ECG) signals were collected, allowing for the analysis of time-frequency domain features in EEG and ECG signals, as well as Heart Rate Variability (HRV) features in ECG signals. Our findings indicate that specific aromatic gases demonstrate effectiveness in reducing mental fatigue. Furthermore, we employed the Support Vector Machine (SVM) algorithm to classify the state of human mental fatigue. Based on the classification results, the release of aromatic gas was controlled to provide targeted aromatic feedback. This innovative approach offers a promising avenue for objectively assessing and addressing mental fatigue through aromatherapy interventions.

          </div>
        

        

        
      
    </div>
  </div>
</div>

<div class="citation-container">
  <div class="citation">
    
      <a href="https://doi.org/g87m3d" class="citation-image" aria-label="The Three-Lead EEG Sensor: Introducing an EEG-Assisted Depression Diagnosis System Based on Ant Lion Optimization">
        <img src="/images/tian_2.jpg" alt="The Three-Lead EEG Sensor: Introducing an EEG-Assisted Depression Diagnosis System Based on Ant Lion Optimization" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
      </a>
    

    <div class="citation-text">
      
      

      <a href="https://doi.org/g87m3d" class="citation-title">
        The Three-Lead EEG Sensor: Introducing an EEG-Assisted Depression Diagnosis System Based on Ant Lion Optimization
      </a>

      <div class="citation-authors" tabindex="0">
        Fuze Tian, Lixian Zhu, Qiuxia Shi, Rui Wang, Lixin Zhang, Qunxi Dong, Kun Qian, Qinglin Zhao, Bin Hu

      </div>

      <div class="citation-details">
        <span class="citation-publisher">IEEE Transactions on Biomedical Circuits and Systems</span>
         · 
        <span class="citation-date">01 Dec 2023</span>
         · 
        <span class="citation-id">doi:10.1109/TBCAS.2023.3292237</span>
      </div>

      
        
          <div class="citation-description">
            For depression diagnosis, traditional methods such as interviews and clinical scales have been widely leveraged in the past few decades, but they are subjective, time-consuming, and labor-consuming. With the development of affective computing and Artificial Intelligence (AI) technologies, Electroencephalogram (EEG)-based depression detection methods have emerged. However, previous research has virtually neglected practical application scenarios, as most studies have focused on analyzing and modeling EEG data. Furthermore, EEG data is typically obtained from specialized devices that are large, complex to operate, and poorly ubiquitous. To address these challenges, a wearable three-lead EEG sensor with flexible electrodes was developed to obtain prefrontal-lobe EEG data. Experimental measurements show that the EEG sensor achieves promising performance (background noise of no more than 0.91 μVpp, Signal-to-Noise Ratio (SNR) of 26–48 dB, and electrode-skin contact impedance of less than 1 KΩ). In addition, EEG data from 70 depressed patients and 108 healthy controls were collected using the EEG sensor, and the linear and nonlinear features were extracted. The features were then weighted and selected using the Ant Lion Optimization (ALO) algorithm to improve classification performance. The experimental results show that the k-NN classifier achieves a classification accuracy of 90.70%, specificity of 96.53%, and sensitivity of 81.79%, indicating the promising potential of the three-lead EEG sensor combined with the ALO algorithm and the k-NN classifier for EEG-assisted depression diagnosis.

          </div>
        

        

        
      
    </div>
  </div>
</div>

<div class="citation-container">
  <div class="citation">
    
      <a href="https://doi.org/g86jjc" class="citation-image" aria-label="Multi-Track Music Generation with WGAN-GP and Attention Mechanisms">
        <img src="/images/GCCE_2023.jpg" alt="Multi-Track Music Generation with WGAN-GP and Attention Mechanisms" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
      </a>
    

    <div class="citation-text">
      
      

      <a href="https://doi.org/g86jjc" class="citation-title">
        Multi-Track Music Generation with WGAN-GP and Attention Mechanisms
      </a>

      <div class="citation-authors" tabindex="0">
        Luyu Chen, Lin Shen, Dan Yu, Zhihua Wang, Kun Qian, Bin Hu, Björn W. Schuller, Yoshiharu Yamamoto

      </div>

      <div class="citation-details">
        <span class="citation-publisher">2023 IEEE 12th Global Conference on Consumer Electronics (GCCE)</span>
         · 
        <span class="citation-date">10 Oct 2023</span>
         · 
        <span class="citation-id">doi:10.1109/GCCE59613.2023.10315503</span>
      </div>

      
        
          <div class="citation-description">
            Music generation with artificial intelligence is a complex and captivating task. 
The utilisation of generative adversarial networks (GANs) has exhibited promising outcomes in producing realistic and diverse music compositions. 
In this paper, we propose a model based on Wasserstein GAN with gradient penalty (WGAN-GP) for multi-track music generation. 
This model incorporates self-attention and introduces a novel cross-attention mechanism in the generator to enhance its expressive capability. 
Additionally, we transpose all music to C major in training to ensure data consistency and quality. 
Experimental results demonstrate that our model can produce multi-track music with enhanced rhythm and sound characteristics, accelerate convergence, and improve generation quality.

          </div>
        

        

        
      
    </div>
  </div>
</div>

<div class="citation-container">
  <div class="citation">
    
      <a href="https://doi.org/g86v6v" class="citation-image" aria-label="Less is More: A Novel Feature Extraction Method for Heart Sound Classification via Fractal Transformation">
        <img src="/images/EMBC_2023_zhu.jpg" alt="Less is More: A Novel Feature Extraction Method for Heart Sound Classification via Fractal Transformation" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
      </a>
    

    <div class="citation-text">
      
      

      <a href="https://doi.org/g86v6v" class="citation-title">
        Less is More: A Novel Feature Extraction Method for Heart Sound Classification via Fractal Transformation
      </a>

      <div class="citation-authors" tabindex="0">
        Cuiping Zhu, Zhonghao Zhao, Yang Tan, Mengkai Sun, Kun Qian, Tao Jiang, Bin Hu, Björn W. Schuller, Yoshiharu Yamamoto

      </div>

      <div class="citation-details">
        <span class="citation-publisher">2023 45th Annual International Conference of the IEEE Engineering in Medicine &amp; Biology Society (EMBC)</span>
         · 
        <span class="citation-date">24 Jul 2023</span>
         · 
        <span class="citation-id">doi:10.1109/EMBC40787.2023.10340710</span>
      </div>

      
        
          <div class="citation-description">
            Cardiovascular diseases (CVDs) are the leading cause of death globally. Heart sound signal analysis plays an important role in clinical detection and physical examination of CVDs. In recent years, auxiliary diagnosis technology of CVDs based on the detection of heart sound signals has become a research hotspot. The detection of abnormal heart sounds can provide important clinical information to help doctors diagnose and treat heart disease. We propose a new set of fractal features-fractal dimension (FD)-as the representation for classification and a Support Vector Machine (SVM) as the classification model. The whole process of the method includes cutting heart sounds, feature extraction, and classification of abnormal heart sounds. We compare the classification results of the heart sound waveform (time domain) and the spectrum (frequency domain) based on fractal features. Finally, according to the better classification results, we choose the fractal features that are most conducive for classification to obtain better classification performance. The features we propose outperform the widely used features significantly (p &lt; .05 by one-tailed z-test) with a much lower dimension. Clinical relevance-The heart sound classification model based on fractal provides a new time-frequency analysis method for heart sound signals. A new effective mechanism is proposed to explore the relationship between the heart sound acoustic properties and the pathology of CVDs. As a non-invasive diagnostic method, this work could supply an idea for the preliminary screening of cardiac abnormalities through heart sounds.

          </div>
        

        

        
      
    </div>
  </div>
</div>

<div class="citation-container">
  <div class="citation">
    
      <a href="https://doi.org/gshqt3" class="citation-image" aria-label="Intelligent Music Intervention for Mental Disorders: Insights and Perspectives">
        <img src="/images/TCSS_2023.jpg" alt="Intelligent Music Intervention for Mental Disorders: Insights and Perspectives" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
      </a>
    

    <div class="citation-text">
      
      

      <a href="https://doi.org/gshqt3" class="citation-title">
        Intelligent Music Intervention for Mental Disorders: Insights and Perspectives
      </a>

      <div class="citation-authors" tabindex="0">
        Kun Qian, Bjorn W. Schuller, Xiaohong Guan, Bin Hu

      </div>

      <div class="citation-details">
        <span class="citation-publisher">IEEE Transactions on Computational Social Systems</span>
         · 
        <span class="citation-date">01 Feb 2023</span>
         · 
        <span class="citation-id">doi:10.1109/TCSS.2023.3235079</span>
      </div>

      
        
          <div class="citation-description">
            Welcome to the first issue of IEEE Transactions on Computational Social Systems (TCSS) of 2023. 
The past 2022 was again a very productive year, in which we have published 159 articles with about 1850 pages in six issues. 
We also received much great and exciting news.

          </div>
        

        

        
      
    </div>
  </div>
</div>
  </section>


    </main>
    <!-- 显示常用网站链接 -->
    <section class="useful-links">
      <h2>Commonly used website links</h2>
      <div class="link-container">
        
          <a href="https://space.bilibili.com/52108001/?spm_id_from=333.999.0.0" class="link-item">bilibili</a>
        
          <a href="https://www.bit.edu.cn/" class="link-item">Beijing Institute of Technology</a>
        
          <a href="https://smt.bit.edu.cn/" class="link-item">School of Medical Technology</a>
        
          <a href="https://bhe-lab.org/" class="link-item">Key Laboratory of Brain Health Intelligent Evaluation and Intervention of the Ministry of Education</a>
        
      </div>
    </section>
    


<footer class="background" style="--image: url('/images/background.jpg')" data-dark="true" data-size="wide">
  <!--
    <div>
      Extra details like contact info or address
    </div>
  -->

  <div>
    
      
      
      



  <div class="button-wrapper">
    <a class="button" href="https://space.bilibili.com/52108001/?spm_id_from=333.999.0.0" data-style="bare" aria-label="button">
      
      
    </a>
  </div>


    
      
      
      



  <div class="button-wrapper">
    <a class="button" href="https://www.bit.edu.cn/" data-style="bare" aria-label="button">
      
      
    </a>
  </div>


    
      
      
      



  <div class="button-wrapper">
    <a class="button" href="https://smt.bit.edu.cn/" data-style="bare" aria-label="button">
      
      
    </a>
  </div>


    
      
      
      



  <div class="button-wrapper">
    <a class="button" href="https://bhe-lab.org/" data-style="bare" aria-label="button">
      
      
    </a>
  </div>


    
  </div>

  <div>
    © 2025
    BHE-AIM-IMT
      |   Built with
    <a href="https://github.com/greenelab/lab-website-template">
      Lab Website Template
    </a>
  </div>

  <input type="checkbox" class="dark-toggle" data-tooltip="Dark mode" aria-label="toggle dark mode" oninput="onDarkToggleChange(event)">
</footer>

    <style>
      .useful-links {
        margin-top: 20px;
      }
      .link-container {
        display: flex;
        flex-wrap: wrap;
        gap: 15px; /* 链接之间的间距 */
      }
      .link-item {
        color: black;
        text-decoration: none;
      }
      .link-item:hover {
        text-decoration: underline; /* 鼠标悬停时显示下划线 */
      }
    </style>
  </body>
</html>
