<!DOCTYPE html>
<html lang="en" data-dark="false">
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <!--
  put your analytics (e.g. Google Analytics) tracking code here
-->

  <!--
  put your search engine verification (e.g. Google Search Console) tag here
-->

  


























<meta name="viewport" content="width=device-width, initial-scale=1">

<title>Research | BHE-AIM-IMT</title>

<link rel="icon" href="/images/icon.png">

<meta name="title" content="Research">
<meta name="description" content="Welcome to bhe-aim-imt website! Relying on The Key Laboratory of Brain Health Intelligent Evaluation and Intervention of the Ministry of Education, our lab is dedicated to solving the problem of intelligent audio intervention and contributing to the advancement of the discipline and society. We welcome people from all areas to visit our website to know more about our research achievements, team members and other contents. We look forward to working with you to explore the mysteries of science!">

<meta property="og:title" content="Research">
<meta property="og:site_title" content="BHE-AIM-IMT">
<meta property="og:description" content="Welcome to bhe-aim-imt website! Relying on The Key Laboratory of Brain Health Intelligent Evaluation and Intervention of the Ministry of Education, our lab is dedicated to solving the problem of intelligent audio intervention and contributing to the advancement of the discipline and society. We welcome people from all areas to visit our website to know more about our research achievements, team members and other contents. We look forward to working with you to explore the mysteries of science!">
<meta property="og:url" content="https://bhe-aim-imt.github.io">
<meta property="og:image" content="/images/share.jpg">
<meta property="og:locale" content="en_US">

<meta property="twitter:title" content="Research">
<meta property="twitter:description" content="Welcome to bhe-aim-imt website! Relying on The Key Laboratory of Brain Health Intelligent Evaluation and Intervention of the Ministry of Education, our lab is dedicated to solving the problem of intelligent audio intervention and contributing to the advancement of the discipline and society. We welcome people from all areas to visit our website to know more about our research achievements, team members and other contents. We look forward to working with you to explore the mysteries of science!">
<meta property="twitter:url" content="https://bhe-aim-imt.github.io">
<meta property="twitter:card" content="summary_large_image">
<meta property="twitter:image" content="/images/share.jpg">


  <meta property="og:type" content="website">


<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "WebSite",
    
    "name": "Research",
    "description": "Welcome to bhe-aim-imt website! Relying on The Key Laboratory of Brain Health Intelligent Evaluation and Intervention of the Ministry of Education, our lab is dedicated to solving the problem of intelligent audio intervention and contributing to the advancement of the discipline and society. We welcome people from all areas to visit our website to know more about our research achievements, team members and other contents. We look forward to working with you to explore the mysteries of science!",
    "headline": "Research",
    "publisher": {
      "@type": "Organization",
      "logo": { "@type": "ImageObject", "url": "/images/icon.png" }
    },
    "url": "https://bhe-aim-imt.github.io"
  }
</script>

<link rel="alternate" type="application/rss+xml" href="https://bhe-aim-imt.github.io/feed.xml">

  <!-- Google Fonts -->
<!-- automatically get url from fonts used in theme file -->

<link rel="preconnect" href="https://fonts.gstatic.com">
<link href="https://fonts.googleapis.com/css2?display=swap&&family=Barlow:ital,wght@0,200;0,400;0,500;0,600;1,200;1,400;1,500;1,600&amp;family=Roboto+Mono:ital,wght@0,200;0,400;0,500;0,600;1,200;1,400;1,500;1,600" rel="stylesheet">

<!-- Font Awesome icons (load asynchronously due to size) -->

<link href="https://use.fontawesome.com/releases/v6.5.0/css/all.css" rel="stylesheet" media="none" onload="this.removeAttribute('media'); this.onload = null;">
<noscript>
  <link href="https://use.fontawesome.com/releases/v6.5.0/css/all.css" rel="stylesheet">
</noscript>

  <!-- third party styles -->
<!-- https://stylishthemes.github.io/Syntax-Themes/pygments/ -->
<link href="https://cdn.jsdelivr.net/gh/StylishThemes/Syntax-Themes/pygments/css-github/pygments-tomorrow-night-eighties.css" rel="stylesheet">

<!-- include all sass in styles folder -->


  
    <link href="/_styles/-theme.css" rel="stylesheet">
  

  
    <link href="/_styles/alert.css" rel="stylesheet">
  

  
    <link href="/_styles/all.css" rel="stylesheet">
  

  
    <link href="/_styles/anchor.css" rel="stylesheet">
  

  
    <link href="/_styles/background.css" rel="stylesheet">
  

  
    <link href="/_styles/body.css" rel="stylesheet">
  

  
    <link href="/_styles/bold.css" rel="stylesheet">
  

  
    <link href="/_styles/button.css" rel="stylesheet">
  

  
    <link href="/_styles/card.css" rel="stylesheet">
  

  
    <link href="/_styles/checkbox.css" rel="stylesheet">
  

  
    <link href="/_styles/citation.css" rel="stylesheet">
  

  
    <link href="/_styles/code.css" rel="stylesheet">
  

  
    <link href="/_styles/cols.css" rel="stylesheet">
  

  
    <link href="/_styles/dark-toggle.css" rel="stylesheet">
  

  
    <link href="/_styles/details.css" rel="stylesheet">
  

  
    <link href="/_styles/feature.css" rel="stylesheet">
  

  
    <link href="/_styles/figure.css" rel="stylesheet">
  

  
    <link href="/_styles/float.css" rel="stylesheet">
  

  
    <link href="/_styles/font.css" rel="stylesheet">
  

  
    <link href="/_styles/footer.css" rel="stylesheet">
  

  
    <link href="/_styles/form.css" rel="stylesheet">
  

  
    <link href="/_styles/grid.css" rel="stylesheet">
  

  
    <link href="/_styles/header.css" rel="stylesheet">
  

  
    <link href="/_styles/heading.css" rel="stylesheet">
  

  
    <link href="/_styles/highlight.css" rel="stylesheet">
  

  
    <link href="/_styles/icon.css" rel="stylesheet">
  

  
    <link href="/_styles/image.css" rel="stylesheet">
  

  
    <link href="/_styles/link.css" rel="stylesheet">
  

  
    <link href="/_styles/list.css" rel="stylesheet">
  

  
    <link href="/_styles/main.css" rel="stylesheet">
  

  
    <link href="/_styles/paragraph.css" rel="stylesheet">
  

  
    <link href="/_styles/portrait.css" rel="stylesheet">
  

  
    <link href="/_styles/post-excerpt.css" rel="stylesheet">
  

  
    <link href="/_styles/post-info.css" rel="stylesheet">
  

  
    <link href="/_styles/post-nav.css" rel="stylesheet">
  

  
    <link href="/_styles/quote.css" rel="stylesheet">
  

  
    <link href="/_styles/rule.css" rel="stylesheet">
  

  
    <link href="/_styles/search-box.css" rel="stylesheet">
  

  
    <link href="/_styles/search-info.css" rel="stylesheet">
  

  
    <link href="/_styles/section.css" rel="stylesheet">
  

  
    <link href="/_styles/table.css" rel="stylesheet">
  

  
    <link href="/_styles/tags.css" rel="stylesheet">
  

  
    <link href="/_styles/textbox.css" rel="stylesheet">
  

  
    <link href="/_styles/tooltip.css" rel="stylesheet">
  

  
    <link href="/_styles/util.css" rel="stylesheet">
  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  


<!-- include all css in styles folder -->



  <!-- third party scripts -->
<script src="https://unpkg.com/@popperjs/core@2" defer></script>
<script src="https://unpkg.com/tippy.js@6" defer></script>
<script src="https://unpkg.com/mark.js@8" defer></script>

<!-- include all js in scripts folder -->


  <script src="/_scripts/anchors.js"></script>

  <script src="/_scripts/dark-mode.js"></script>

  <script src="/_scripts/fetch-tags.js"></script>

  <script src="/_scripts/search.js"></script>

  <script src="/_scripts/site-search.js"></script>

  <script src="/_scripts/table-wrap.js"></script>

  <script src="/_scripts/tooltip.js"></script>


</head>

  <body>
    







<header class="background" style="--image: url('/images/background_brain.jpg')" data-dark="true">
  <a href="/" class="home">
    
      <span class="logo">
        
          <svg xmlns="http://www.w3.org/2000/svg" viewbox="-40 -60 80 100">
  <style>
    .bubble {
      animation: float 2s ease-out both infinite var(--delay);
    }
    @keyframes float {
      0% {
        opacity: 0;
      }
      50% {
        transform: translateY(0);
        opacity: 0;
      }
      75% {
        opacity: 1;
      }
      100% {
        opacity: 0;
        transform: translateY(-40px);
      }
    }
  </style>
  <g fill="currentColor" opacity="0.5">
    <circle class="bubble" cx="0" cy="-10" r="3" style="--delay: 0.1s"></circle>
    <circle class="bubble" cx="0" cy="-10" r="3" style="--delay: 0.4s"></circle>
    <circle class="bubble" cx="0" cy="-10" r="3" style="--delay: 1.1s"></circle>
  </g>
  <path fill="#38bdf8" d="
      M 0 -22.5
      L -19.5 -11.25
      L -19.5 11.25
      L 0 22.5
      L 19.5 11.25
      L 19.5 -11.25
      z
    "></path>
  <path fill="#bae6fd" d="
      M 0 -22.5
      L -19.5 -11.25
      L 0 0
      L 19.5 -11.25
      z
    "></path>
  <path fill="none" stroke="currentColor" stroke-width="5" d="
      M -18 -53
      L -10 -53
      L -10 -29.2
      L -30.3 -17.5
      L -30.3 17.5
      L 0 35
      L 30.3 17.5
      L 30.3 -17.5
      L 10 -29.2
      L 10 -53
      L 18 -53
    "></path>
</svg>

        
      </span>
    
    
      <span class="title-text" data-tooltip="Home">
        
          <span class="title">BHE-AIM-IMT</span>
        
        
      </span>
    
  </a>

  <input class="nav-toggle" type="checkbox" aria-label="show/hide nav">

  <nav>
    
    
      
        <a href="/research/" data-tooltip="Published works">
          Research
        </a>
      
    
      
        <a href="/projects/" data-tooltip="Software, datasets, and more">
          Projects
        </a>
      
    
      
        <a href="/" data-tooltip="Software, datasets, and more">
          Thesis
        </a>
      
    
      
        <a href="/team/" data-tooltip="About our team">
          Team
        </a>
      
    
  </nav>
</header>

    <main>
      <!--
  modify main content of page:
  - add section breaks
  - attach section properties
  - filter out blank sections
-->






  
  
  

  <section class="background" data-size="page">
    <h1 id="research">
<i class="icon fa-solid fa-microscope"></i>Research</h1>
  </section>

  
  
  

  <section class="background" data-size="page">
    <!--
  background: ;
  dark: ;
  size: ;
-->

<h2 id="all">All</h2>

<div class="search-box">
  <input type="text" class="search-input" oninput="onSearchInput(this)" placeholder="Search items on this page">
  <button disabled data-tooltip="Clear search" aria-label="clear search" onclick="onSearchClear()">
    <i class="icon fa-solid fa-magnifying-glass"></i>
  </button>
</div>

<div class="search-info"></div>

<h3 id="2025">2025</h3>

<div class="citation-container">
  <div class="citation">
    
      <a href="https://doi.org/g864n5" class="citation-image" aria-label="An AI-assisted All-in-one Integrated Coronary Artery Disease Diagnosis System Using a Portable Heart Sound Sensor with an On-board Executable Lightweight Model">
        <img src="/images/TMC_2025.jpg" alt="An AI-assisted All-in-one Integrated Coronary Artery Disease Diagnosis System Using a Portable Heart Sound Sensor with an On-board Executable Lightweight Model" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
      </a>
    

    <div class="citation-text">
      
      

      <a href="https://doi.org/g864n5" class="citation-title">
        An AI-assisted All-in-one Integrated Coronary Artery Disease Diagnosis System Using a Portable Heart Sound Sensor with an On-board Executable Lightweight Model
      </a>

      <div class="citation-authors" data-tooltip="Haojie Zhang, Fuze Tian, Yang Tan, Lin Shen, Jingyu Liu, Jie Liu, Kun Qian, Yalei Han, Gong Su, Bin Hu, Björn W. Schuller, Yoshiharu Yamamoto" tabindex="0">
        Haojie Zhang, Fuze Tian, Yang Tan, Lin Shen, Jingyu Liu, …, Yalei Han, Gong Su, Bin Hu, Björn W. Schuller, Yoshiharu Yamamoto

      </div>

      <div class="citation-details">
        <span class="citation-publisher">IEEE Transactions on Mobile Computing</span>
         · 
        <span class="citation-date">01 Jan 2025</span>
         · 
        <span class="citation-id">doi:10.1109/TMC.2025.3547842</span>
      </div>

      
        
          <div class="citation-description">
            Heart sounds play a crucial role in assessing Coronary Artery Disease (CAD). The advancement of Artificial Intelligence (AI) technologies has given rise to Computer Audition (CA)-based methods for CAD detection. However, previous research has focused primarily on analyzing and modeling heart sound data, overlooking practical application scenarios. In this work, we design a pervasive heart sound collection device used for high-quality heart sound data acquisition. Moreover, we introduce an on-board executable lightweight network tailored for the designed portable device, referred to as TYKDModel. Further, heart sound data from 41 CAD patients and 22 non-CAD healthy controls are collected using the developed device. Experimental results show that the TYKDModel exhibits low-computational complexity, with 52.16 K parameters and 5.03 M Floating-Point Operations (FLOPs). When deployed on the board, it requires only 1.10 MB of Random Access Memory (RAM) and 236.27 KB of Read-Only Memory (ROM), and takes around 1.72 seconds to perform a classification. Despite the low computational and spatial complexity, the TYKDModel achieves a notable classification accuracy of 85.2%, specificity of 88.6%, and sensitivity of 82.8% on the board. These results indicate the promising potential of AI-assisted all-in-one integrated system for the diagnosis of heart sound-assisted CAD.

          </div>
        

        

        
      
    </div>
  </div>
</div>

<div class="citation-container">
  <div class="citation">
    
      <a href="https://doi.org/g86jh9" class="citation-image" aria-label="An On-Board Executable Multi-Feature Transfer-Enhanced Fusion Model for Three-Lead EEG Sensor-Assisted Depression Diagnosis">
        <img src="/images/JBHI_2024.jpg" alt="An On-Board Executable Multi-Feature Transfer-Enhanced Fusion Model for Three-Lead EEG Sensor-Assisted Depression Diagnosis" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
      </a>
    

    <div class="citation-text">
      
      

      <a href="https://doi.org/g86jh9" class="citation-title">
        An On-Board Executable Multi-Feature Transfer-Enhanced Fusion Model for Three-Lead EEG Sensor-Assisted Depression Diagnosis
      </a>

      <div class="citation-authors" tabindex="0">
        Fuze Tian, Haojie Zhang, Yang Tan, Lixian Zhu, Lin Shen, Kun Qian, Bin Hu, Björn W. Schuller, Yoshiharu Yamamoto

      </div>

      <div class="citation-details">
        <span class="citation-publisher">IEEE Journal of Biomedical and Health Informatics</span>
         · 
        <span class="citation-date">01 Jan 2025</span>
         · 
        <span class="citation-id">doi:10.1109/JBHI.2024.3487012</span>
      </div>

      
        
          <div class="citation-description">
            The development of affective computing and medical electronic technologies has led to the emergence of Artificial Intelligence (AI)-based methods for the early detection of depression. 
However, previous studies have often overlooked the necessity for the AI-assisted diagnosis system to be wearable and accessible in practical scenarios for depression recognition. 
In this work, we present an on-board executable multi-feature transfer-enhanced fusion model for our custom-designed wearable three-lead Electroencephalogram (EEG) sensor, based on EEG data collected from 73 depressed patients and 108 healthy controls. 
Experimental results show that the proposed model exhibits low-computational complexity (65.0 K parameters), promising Floating-Point Operations (FLOPs) performance (25.6 M), real-time processing (1.5 s/execution), and low power consumption (320.8 mW). 
Furthermore, it requires only 202.0 KB of Random Access Memory (RAM) and 279.6 KB of Read-Only Memory (ROM) when deployed on the EEG sensor. 
Despite its low computational and spatial complexity, the model achieves a notable classification accuracy of 95.2%, specificity of 94.0%, and sensitivity of 96.9% under independent test conditions. 
These results underscore the potential of deploying the model on the wearable three-lead EEG sensor for assisting in the diagnosis of depression.

          </div>
        

        

        
      
    </div>
  </div>
</div>

<h3 id="2024">2024</h3>

<div class="citation-container">
  <div class="citation">
    
      <a href="https://doi.org/g87m3f" class="citation-image" aria-label="Advancements in Affective Disorder Detection: Using Multimodal Physiological Signals and Neuromorphic Computing Based on SNNs">
        <img src="/images/tian_3.jpg" alt="Advancements in Affective Disorder Detection: Using Multimodal Physiological Signals and Neuromorphic Computing Based on SNNs" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
      </a>
    

    <div class="citation-text">
      
      

      <a href="https://doi.org/g87m3f" class="citation-title">
        Advancements in Affective Disorder Detection: Using Multimodal Physiological Signals and Neuromorphic Computing Based on SNNs
      </a>

      <div class="citation-authors" tabindex="0">
        Fuze Tian, Lixin Zhang, Lixian Zhu, Mingqi Zhao, Jingyu Liu, Qunxi Dong, Qinglin Zhao

      </div>

      <div class="citation-details">
        <span class="citation-publisher">IEEE Transactions on Computational Social Systems</span>
         · 
        <span class="citation-date">01 Dec 2024</span>
         · 
        <span class="citation-id">doi:10.1109/TCSS.2024.3420445</span>
      </div>

      
        
          <div class="citation-description">
            Currently, the integration of artificial intelligence (AI) techniques with multimodal physiological signals represents a pivotal approach to detect affective disorders (ADs). With the increasing complexity and diversity of physiological signal modalities, researchers have introduced various AI methods using multimodal physiological signals to improve model classification performance and explainability to increase trust and facilitate clinical adoption. Among these methods, spiking neural networks (SNNs) stand out as a promising avenue due to their alignment with the operating principles of the human brain, robust biological explainability, and adeptness in processing spatial–temporal information in an efficient event-driven manner with low power consumption. Furthermore, the emergence of neuromorphic computing (NC) chips based on SNNs has greatly bolstered the field of NC, enabling effective support for objective, pervasive, and wearable AI-assisted medical diagnostic devices for ADs and other diseases. This article presents a review of recent achievements in multimodal AD detection and points out the associated challenges in utilizing multimodal physiological signals and NC based on SNNs for AD detection. Building upon this foundation, we give perspectives on future work. The intended readership for this review consists of researchers in the fields of cognitive computing, computational psychophysiology, affective computing, NC, and brain-inspired computing. We hope that this survey not only garners increased attention from the scientific community but also serves as a valuable guide for future studies in this field.

          </div>
        

        

        
      
    </div>
  </div>
</div>

<div class="citation-container">
  <div class="citation">
    
      <a href="https://doi.org/g86jjb" class="citation-image" aria-label="A First Look at Generative Artificial Intelligence Based Music Therapy for Mental Disorders">
        <img src="/images/TCE_2024.jpg" alt="A First Look at Generative Artificial Intelligence Based Music Therapy for Mental Disorders" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
      </a>
    

    <div class="citation-text">
      
      

      <a href="https://doi.org/g86jjb" class="citation-title">
        A First Look at Generative Artificial Intelligence Based Music Therapy for Mental Disorders
      </a>

      <div class="citation-authors" tabindex="0">
        Lin Shen, Haojie Zhang, Cuiping Zhu, Ruobing Li, Kun Qian, Wei Meng, Fuze Tian, Bin Hu, Björn W. Schuller, Yoshiharu Yamamoto

      </div>

      <div class="citation-details">
        <span class="citation-publisher">IEEE Transactions on Consumer Electronics</span>
         · 
        <span class="citation-date">01 Dec 2024</span>
         · 
        <span class="citation-id">doi:10.1109/TCE.2024.3514633</span>
      </div>

      
        
          <div class="citation-description">
            Mental disorders show a rapid increase and cause considerable harm to individuals as well as the society in recent decade. 
Hence, mental disorders have become a serious public health challenge in nowadays society. 
Timely treatment of mental disorders plays a critical role for reducing the harm of mental illness to individuals and society. 
Music therapy is a type of non-pharmaceutical method in treating such mental disorders. 
However, conventional music therapy suffers from a number of issues resulting in a lack of popularity. 
Thanks to the rapid development of Artificial Intelligence (AI), especially the AI Generated Content (AIGC), it provides a chance to address these issues. Nevertheless, to the best of our knowledge, there is no work investigating music therapy from AIGC and closed-loop perspective. 
In this paper, we summarise some universal music therapy methods and discuss their shortages. Then, we indicate some AIGC techniques, especially the music generation, for their application in music therapy. 
Moreover, we present a closed-loop music therapy system and introduce its implementation details. 
Finally, we discuss some challenges in AIGC-based music therapy with proposing further research direction, and we suggest the potential of this system to become a consumer-grade product for treating mental disorders.

          </div>
        

        

        
      
    </div>
  </div>
</div>

<div class="citation-container">
  <div class="citation">
    
      <a href="https://doi.org/g86jjf" class="citation-image" aria-label="E-ODN: An Emotion Open Deep Network for Generalised and Adaptive Speech Emotion Recognition">
        <img src="/images/ISP_2024.jpg" alt="E-ODN: An Emotion Open Deep Network for Generalised and Adaptive Speech Emotion Recognition" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
      </a>
    

    <div class="citation-text">
      
      

      <a href="https://doi.org/g86jjf" class="citation-title">
        E-ODN: An Emotion Open Deep Network for Generalised and Adaptive Speech Emotion Recognition
      </a>

      <div class="citation-authors" tabindex="0">
        Liuxian Ma, Lin Shen, Ruobing Li, Haojie Zhang, Kun Qian, Bin Hu, Björn W. Schuller, Yoshiharu Yamamoto

      </div>

      <div class="citation-details">
        <span class="citation-publisher">Interspeech 2024</span>
         · 
        <span class="citation-date">01 Sep 2024</span>
         · 
        <span class="citation-id">doi:10.21437/Interspeech.2024-685</span>
      </div>

      
        
          <div class="citation-description">
            Recognising the widest range of emotions possible is a major challenge in the task of Speech Emotion Recognition(SER), especially for complex and mixed emotions. 
However, due to the limited number of emotional types and uneven distribution of data within existing datasets, current SER models are typically trained and used in a narrow range of emotional types. 
In this paper, we propose the Emotion Open Deep Network(E-ODN) model to address this issue. 
Besides, we introduce a novel Open-Set Recognition method that maps sample emotional features into a three-dimensional emotional space. 
The method can infer unknown emotions and initialise new type weights, enabling the model to dynamically learn and infer emerging emotional types. 
The empirical results show that our recognition model outperforms the state-of-the-art(SOTA) models in dealing with multi-type unbalanced data, and it can also perform finer-grained emotion recognition.

          </div>
        

        

        
      
    </div>
  </div>
</div>

<div class="citation-container">
  <div class="citation">
    
      <a href="https://doi.org/g87m3c" class="citation-image" aria-label="An FFT-Based DC Offset Compensation and I Q Imbalance Correction Algorithm for Bioradar Sensors">
        <img src="/images/tian_1.jpg" alt="An FFT-Based DC Offset Compensation and I Q Imbalance Correction Algorithm for Bioradar Sensors" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
      </a>
    

    <div class="citation-text">
      
      

      <a href="https://doi.org/g87m3c" class="citation-title">
        An FFT-Based DC Offset Compensation and I/Q Imbalance Correction Algorithm for Bioradar Sensors
      </a>

      <div class="citation-authors" tabindex="0">
        Fuze Tian, Lixian Zhu, Qiuxia Shi, Xiaokun Jin, Ran Cai, Qunxi Dong, Qinglin Zhao, Bin Hu

      </div>

      <div class="citation-details">
        <span class="citation-publisher">IEEE Transactions on Microwave Theory and Techniques</span>
         · 
        <span class="citation-date">01 Mar 2024</span>
         · 
        <span class="citation-id">doi:10.1109/TMTT.2023.3308190</span>
      </div>

      
        
          <div class="citation-description">
            The challenge of noncontact presentation of human cardiopulmonary activity using a bioradar sensor is to linearly demodulate the Doppler cardiopulmonary diagram (DCD) signal from baseband signals. Arctangent demodulation can perform linear phase demodulation to obtain the DCD signal. However, the high-order harmonics and intermodulation terms (ITs) caused by the time-varying direct current (dc) offset and in-phase and quadrature-phase (I/Q) imbalance in the baseband signals significantly degrade the signal-to-noise ratio (SNR) of the Doppler heartbeat diagram (DHD) signal. In this work, a fast Fourier transform (FFT)-based algorithm is proposed to simultaneously perform time-varying dc offset compensation and I/Q imbalance correction without the need for an auxiliary device to improve the accuracy of the arctangent demodulation. The obtained results show that the SNRs of the algorithm-processed DHD signals are increased from 30.08 ± 2.41 to 68.88 ± 10.57 dB. In addition, the root mean square errors (RMSEs) of the C-C intervals of the DHD signals for eight subjects with respect to the J-J intervals of the ballistocardiogram (BCG) signals are 17.79 ± 2.72 ms (2.80% ± 0.43%), suggesting a promising potential of the DHD signal for noncontact biomedical applications.

          </div>
        

        

        
      
    </div>
  </div>
</div>

<h3 id="2023">2023</h3>

<div class="citation-container">
  <div class="citation">
    
      <a href="https://doi.org/g87m3h" class="citation-image" aria-label="Design and Verification of an Aromatherapy Feedback System for Mental Fatigue Based on Physiological Signals">
        <img src="/images/tian_5.jpg" alt="Design and Verification of an Aromatherapy Feedback System for Mental Fatigue Based on Physiological Signals" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
      </a>
    

    <div class="citation-text">
      
      

      <a href="https://doi.org/g87m3h" class="citation-title">
        Design and Verification of an Aromatherapy Feedback System for Mental Fatigue Based on Physiological Signals
      </a>

      <div class="citation-authors" tabindex="0">
        Tao Sun, Fuze Tian, Hua Jiang, Qinglin Zhao, Bin Hu

      </div>

      <div class="citation-details">
        <span class="citation-publisher">2023 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)</span>
         · 
        <span class="citation-date">05 Dec 2023</span>
         · 
        <span class="citation-id">doi:10.1109/BIBM58861.2023.10385577</span>
      </div>

      
        
          <div class="citation-description">
            Mental fatigue is a prevalent issue in contemporary society and can negatively affect physical performance and concentration, increasing the likelihood of adverse consequences due to inattention during productive activities. Therefore, it becomes increasingly important to address and eliminate fatigue within a specific period of time. Aromatherapy, as a form of Complementary Alternative Medicine (CAM), is a non-invasive, cost-effective, and efficient method to combat fatigue. Previous studies have assessed the effects of specific aromatherapy oils using scales, but there is a lack of objective and reliable physiological indicators to prove the effectiveness of aromatherapy. Hence, this paper seeks to establish a model illustrating the effects of aromatic essential oil gases on the human body. A multimodal physiological fatigue signal acquisition system that integrates aromatherapy feedback was designed. In addition, an experimental paradigm was developed to explore the potential of aromatherapy in mitigating mental fatigue. Electroencephalogram (EEG) and Electrocardiogram (ECG) signals were collected, allowing for the analysis of time-frequency domain features in EEG and ECG signals, as well as Heart Rate Variability (HRV) features in ECG signals. Our findings indicate that specific aromatic gases demonstrate effectiveness in reducing mental fatigue. Furthermore, we employed the Support Vector Machine (SVM) algorithm to classify the state of human mental fatigue. Based on the classification results, the release of aromatic gas was controlled to provide targeted aromatic feedback. This innovative approach offers a promising avenue for objectively assessing and addressing mental fatigue through aromatherapy interventions.

          </div>
        

        

        
      
    </div>
  </div>
</div>

<div class="citation-container">
  <div class="citation">
    
      <a href="https://doi.org/g87m3d" class="citation-image" aria-label="The Three-Lead EEG Sensor: Introducing an EEG-Assisted Depression Diagnosis System Based on Ant Lion Optimization">
        <img src="/images/tian_2.jpg" alt="The Three-Lead EEG Sensor: Introducing an EEG-Assisted Depression Diagnosis System Based on Ant Lion Optimization" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
      </a>
    

    <div class="citation-text">
      
      

      <a href="https://doi.org/g87m3d" class="citation-title">
        The Three-Lead EEG Sensor: Introducing an EEG-Assisted Depression Diagnosis System Based on Ant Lion Optimization
      </a>

      <div class="citation-authors" tabindex="0">
        Fuze Tian, Lixian Zhu, Qiuxia Shi, Rui Wang, Lixin Zhang, Qunxi Dong, Kun Qian, Qinglin Zhao, Bin Hu

      </div>

      <div class="citation-details">
        <span class="citation-publisher">IEEE Transactions on Biomedical Circuits and Systems</span>
         · 
        <span class="citation-date">01 Dec 2023</span>
         · 
        <span class="citation-id">doi:10.1109/TBCAS.2023.3292237</span>
      </div>

      
        
          <div class="citation-description">
            For depression diagnosis, traditional methods such as interviews and clinical scales have been widely leveraged in the past few decades, but they are subjective, time-consuming, and labor-consuming. With the development of affective computing and Artificial Intelligence (AI) technologies, Electroencephalogram (EEG)-based depression detection methods have emerged. However, previous research has virtually neglected practical application scenarios, as most studies have focused on analyzing and modeling EEG data. Furthermore, EEG data is typically obtained from specialized devices that are large, complex to operate, and poorly ubiquitous. To address these challenges, a wearable three-lead EEG sensor with flexible electrodes was developed to obtain prefrontal-lobe EEG data. Experimental measurements show that the EEG sensor achieves promising performance (background noise of no more than 0.91 μVpp, Signal-to-Noise Ratio (SNR) of 26–48 dB, and electrode-skin contact impedance of less than 1 KΩ). In addition, EEG data from 70 depressed patients and 108 healthy controls were collected using the EEG sensor, and the linear and nonlinear features were extracted. The features were then weighted and selected using the Ant Lion Optimization (ALO) algorithm to improve classification performance. The experimental results show that the k-NN classifier achieves a classification accuracy of 90.70%, specificity of 96.53%, and sensitivity of 81.79%, indicating the promising potential of the three-lead EEG sensor combined with the ALO algorithm and the k-NN classifier for EEG-assisted depression diagnosis.

          </div>
        

        

        
      
    </div>
  </div>
</div>

<div class="citation-container">
  <div class="citation">
    
      <a href="https://doi.org/g86jjc" class="citation-image" aria-label="Multi-Track Music Generation with WGAN-GP and Attention Mechanisms">
        <img src="/images/GCCE_2023.jpg" alt="Multi-Track Music Generation with WGAN-GP and Attention Mechanisms" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
      </a>
    

    <div class="citation-text">
      
      

      <a href="https://doi.org/g86jjc" class="citation-title">
        Multi-Track Music Generation with WGAN-GP and Attention Mechanisms
      </a>

      <div class="citation-authors" tabindex="0">
        Luyu Chen, Lin Shen, Dan Yu, Zhihua Wang, Kun Qian, Bin Hu, Björn W. Schuller, Yoshiharu Yamamoto

      </div>

      <div class="citation-details">
        <span class="citation-publisher">2023 IEEE 12th Global Conference on Consumer Electronics (GCCE)</span>
         · 
        <span class="citation-date">10 Oct 2023</span>
         · 
        <span class="citation-id">doi:10.1109/GCCE59613.2023.10315503</span>
      </div>

      
        
          <div class="citation-description">
            Music generation with artificial intelligence is a complex and captivating task. 
The utilisation of generative adversarial networks (GANs) has exhibited promising outcomes in producing realistic and diverse music compositions. 
In this paper, we propose a model based on Wasserstein GAN with gradient penalty (WGAN-GP) for multi-track music generation. 
This model incorporates self-attention and introduces a novel cross-attention mechanism in the generator to enhance its expressive capability. 
Additionally, we transpose all music to C major in training to ensure data consistency and quality. 
Experimental results demonstrate that our model can produce multi-track music with enhanced rhythm and sound characteristics, accelerate convergence, and improve generation quality.

          </div>
        

        

        
      
    </div>
  </div>
</div>

<div class="citation-container">
  <div class="citation">
    
      <a href="https://doi.org/g86v6v" class="citation-image" aria-label="Less is More: A Novel Feature Extraction Method for Heart Sound Classification via Fractal Transformation">
        <img src="/images/EMBC_2023_zhu.jpg" alt="Less is More: A Novel Feature Extraction Method for Heart Sound Classification via Fractal Transformation" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
      </a>
    

    <div class="citation-text">
      
      

      <a href="https://doi.org/g86v6v" class="citation-title">
        Less is More: A Novel Feature Extraction Method for Heart Sound Classification via Fractal Transformation
      </a>

      <div class="citation-authors" tabindex="0">
        Cuiping Zhu, Zhonghao Zhao, Yang Tan, Mengkai Sun, Kun Qian, Tao Jiang, Bin Hu, Björn W. Schuller, Yoshiharu Yamamoto

      </div>

      <div class="citation-details">
        <span class="citation-publisher">2023 45th Annual International Conference of the IEEE Engineering in Medicine &amp; Biology Society (EMBC)</span>
         · 
        <span class="citation-date">24 Jul 2023</span>
         · 
        <span class="citation-id">doi:10.1109/EMBC40787.2023.10340710</span>
      </div>

      
        
          <div class="citation-description">
            Cardiovascular diseases (CVDs) are the leading cause of death globally. Heart sound signal analysis plays an important role in clinical detection and physical examination of CVDs. In recent years, auxiliary diagnosis technology of CVDs based on the detection of heart sound signals has become a research hotspot. The detection of abnormal heart sounds can provide important clinical information to help doctors diagnose and treat heart disease. We propose a new set of fractal features-fractal dimension (FD)-as the representation for classification and a Support Vector Machine (SVM) as the classification model. The whole process of the method includes cutting heart sounds, feature extraction, and classification of abnormal heart sounds. We compare the classification results of the heart sound waveform (time domain) and the spectrum (frequency domain) based on fractal features. Finally, according to the better classification results, we choose the fractal features that are most conducive for classification to obtain better classification performance. The features we propose outperform the widely used features significantly (p &lt; .05 by one-tailed z-test) with a much lower dimension. Clinical relevance-The heart sound classification model based on fractal provides a new time-frequency analysis method for heart sound signals. A new effective mechanism is proposed to explore the relationship between the heart sound acoustic properties and the pathology of CVDs. As a non-invasive diagnostic method, this work could supply an idea for the preliminary screening of cardiac abnormalities through heart sounds.

          </div>
        

        

        
      
    </div>
  </div>
</div>

<div class="citation-container">
  <div class="citation">
    
      <a href="https://doi.org/gshqt3" class="citation-image" aria-label="Intelligent Music Intervention for Mental Disorders: Insights and Perspectives">
        <img src="/images/TCSS_2023.jpg" alt="Intelligent Music Intervention for Mental Disorders: Insights and Perspectives" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
      </a>
    

    <div class="citation-text">
      
      

      <a href="https://doi.org/gshqt3" class="citation-title">
        Intelligent Music Intervention for Mental Disorders: Insights and Perspectives
      </a>

      <div class="citation-authors" tabindex="0">
        Kun Qian, Bjorn W. Schuller, Xiaohong Guan, Bin Hu

      </div>

      <div class="citation-details">
        <span class="citation-publisher">IEEE Transactions on Computational Social Systems</span>
         · 
        <span class="citation-date">01 Feb 2023</span>
         · 
        <span class="citation-id">doi:10.1109/TCSS.2023.3235079</span>
      </div>

      
        
          <div class="citation-description">
            Welcome to the first issue of IEEE Transactions on Computational Social Systems (TCSS) of 2023. 
The past 2022 was again a very productive year, in which we have published 159 articles with about 1850 pages in six issues. 
We also received much great and exciting news.

          </div>
        

        

        
      
    </div>
  </div>
</div>
  </section>


    </main>
    <!-- 显示常用网站链接 -->
    <section class="useful-links">
      <h2>Commonly used website links</h2>
      <div class="link-container">
        
          <a href="https://space.bilibili.com/52108001/?spm_id_from=333.999.0.0" class="link-item">bilibili</a>
        
          <a href="https://www.bit.edu.cn/" class="link-item">Beijing Institute of Technology</a>
        
          <a href="https://smt.bit.edu.cn/" class="link-item">School of Medical Technology</a>
        
          <a href="https://bhe-lab.org/" class="link-item">Key Laboratory of Brain Health Intelligent Evaluation and Intervention of the Ministry of Education</a>
        
      </div>
    </section>
    


<footer class="background" style="--image: url('/images/background.jpg')" data-dark="true" data-size="wide">
  <!--
    <div>
      Extra details like contact info or address
    </div>
  -->

  <div>
    
      
      
      



  <div class="button-wrapper">
    <a class="button" href="https://space.bilibili.com/52108001/?spm_id_from=333.999.0.0" data-style="bare" aria-label="button">
      
      
    </a>
  </div>


    
      
      
      



  <div class="button-wrapper">
    <a class="button" href="https://www.bit.edu.cn/" data-style="bare" aria-label="button">
      
      
    </a>
  </div>


    
      
      
      



  <div class="button-wrapper">
    <a class="button" href="https://smt.bit.edu.cn/" data-style="bare" aria-label="button">
      
      
    </a>
  </div>


    
      
      
      



  <div class="button-wrapper">
    <a class="button" href="https://bhe-lab.org/" data-style="bare" aria-label="button">
      
      
    </a>
  </div>


    
  </div>

  <div>
    © 2025
    BHE-AIM-IMT
      |   Built with
    <a href="https://github.com/greenelab/lab-website-template">
      Lab Website Template
    </a>
  </div>

  <input type="checkbox" class="dark-toggle" data-tooltip="Dark mode" aria-label="toggle dark mode" oninput="onDarkToggleChange(event)">
</footer>

    <style>
      .useful-links {
        margin-top: 20px;
      }
      .link-container {
        display: flex;
        flex-wrap: wrap;
        gap: 15px; /* 链接之间的间距 */
      }
      .link-item {
        color: black;
        text-decoration: none;
      }
      .link-item:hover {
        text-decoration: underline; /* 鼠标悬停时显示下划线 */
      }
    </style>
  </body>
</html>
