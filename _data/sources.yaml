- id: doi:10.1109/JBHI.2024.3487012
  description: |
    The development of affective computing and medical electronic technologies has led to the emergence of Artificial Intelligence (AI)-based methods for the early detection of depression. 
    However, previous studies have often overlooked the necessity for the AI-assisted diagnosis system to be wearable and accessible in practical scenarios for depression recognition. 
    In this work, we present an on-board executable multi-feature transfer-enhanced fusion model for our custom-designed wearable three-lead Electroencephalogram (EEG) sensor, based on EEG data collected from 73 depressed patients and 108 healthy controls. 
    Experimental results show that the proposed model exhibits low-computational complexity (65.0 K parameters), promising Floating-Point Operations (FLOPs) performance (25.6 M), real-time processing (1.5 s/execution), and low power consumption (320.8 mW). 
    Furthermore, it requires only 202.0 KB of Random Access Memory (RAM) and 279.6 KB of Read-Only Memory (ROM) when deployed on the EEG sensor. 
    Despite its low computational and spatial complexity, the model achieves a notable classification accuracy of 95.2%, specificity of 94.0%, and sensitivity of 96.9% under independent test conditions. 
    These results underscore the potential of deploying the model on the wearable three-lead EEG sensor for assisting in the diagnosis of depression.
  image: images/JBHI_2024.jpg

- id: doi:10.1109/TCE.2024.3514633
  description: |
    Mental disorders show a rapid increase and cause considerable harm to individuals as well as the society in recent decade. 
    Hence, mental disorders have become a serious public health challenge in nowadays society. 
    Timely treatment of mental disorders plays a critical role for reducing the harm of mental illness to individuals and society. 
    Music therapy is a type of non-pharmaceutical method in treating such mental disorders. 
    However, conventional music therapy suffers from a number of issues resulting in a lack of popularity. 
    Thanks to the rapid development of Artificial Intelligence (AI), especially the AI Generated Content (AIGC), it provides a chance to address these issues. Nevertheless, to the best of our knowledge, there is no work investigating music therapy from AIGC and closed-loop perspective. 
    In this paper, we summarise some universal music therapy methods and discuss their shortages. Then, we indicate some AIGC techniques, especially the music generation, for their application in music therapy. 
    Moreover, we present a closed-loop music therapy system and introduce its implementation details. 
    Finally, we discuss some challenges in AIGC-based music therapy with proposing further research direction, and we suggest the potential of this system to become a consumer-grade product for treating mental disorders.
  image: images/TCE_2024.jpg
  date: 2024-12-01

- id: doi:10.1109/TCSS.2023.3235079
  description: |
    Welcome to the first issue of IEEE Transactions on Computational Social Systems (TCSS) of 2023. 
    The past 2022 was again a very productive year, in which we have published 159 articles with about 1850 pages in six issues. 
    We also received much great and exciting news.
  image: images/TCSS_2023.jpg

- id: doi:10.1109/GCCE59613.2023.10315503
  description: |
    Music generation with artificial intelligence is a complex and captivating task. 
    The utilisation of generative adversarial networks (GANs) has exhibited promising outcomes in producing realistic and diverse music compositions. 
    In this paper, we propose a model based on Wasserstein GAN with gradient penalty (WGAN-GP) for multi-track music generation. 
    This model incorporates self-attention and introduces a novel cross-attention mechanism in the generator to enhance its expressive capability. 
    Additionally, we transpose all music to C major in training to ensure data consistency and quality. 
    Experimental results demonstrate that our model can produce multi-track music with enhanced rhythm and sound characteristics, accelerate convergence, and improve generation quality.
  image: images/GCCE_2023.jpg

- id: doi:10.21437/Interspeech.2024-685
  description: |
    Recognising the widest range of emotions possible is a major challenge in the task of Speech Emotion Recognition(SER), especially for complex and mixed emotions. 
    However, due to the limited number of emotional types and uneven distribution of data within existing datasets, current SER models are typically trained and used in a narrow range of emotional types. 
    In this paper, we propose the Emotion Open Deep Network(E-ODN) model to address this issue. 
    Besides, we introduce a novel Open-Set Recognition method that maps sample emotional features into a three-dimensional emotional space. 
    The method can infer unknown emotions and initialise new type weights, enabling the model to dynamically learn and infer emerging emotional types. 
    The empirical results show that our recognition model outperforms the state-of-the-art(SOTA) models in dealing with multi-type unbalanced data, and it can also perform finer-grained emotion recognition.
  image: images/ISP_2024.jpg

- id: doi:10.1109/TMC.2025.3547842
  description: |
    Heart sounds play a crucial role in assessing Coronary Artery Disease (CAD). The advancement of Artificial Intelligence (AI) technologies has given rise to Computer Audition (CA)-based methods for CAD detection. However, previous research has focused primarily on analyzing and modeling heart sound data, overlooking practical application scenarios. In this work, we design a pervasive heart sound collection device used for high-quality heart sound data acquisition. Moreover, we introduce an on-board executable lightweight network tailored for the designed portable device, referred to as TYKDModel. Further, heart sound data from 41 CAD patients and 22 non-CAD healthy controls are collected using the developed device. Experimental results show that the TYKDModel exhibits low-computational complexity, with 52.16 K parameters and 5.03 M Floating-Point Operations (FLOPs). When deployed on the board, it requires only 1.10 MB of Random Access Memory (RAM) and 236.27 KB of Read-Only Memory (ROM), and takes around 1.72 seconds to perform a classification. Despite the low computational and spatial complexity, the TYKDModel achieves a notable classification accuracy of 85.2%, specificity of 88.6%, and sensitivity of 82.8% on the board. These results indicate the promising potential of AI-assisted all-in-one integrated system for the diagnosis of heart sound-assisted CAD.
  image: images/TMC_2025.jpg


- id: doi:10.1109/EMBC40787.2023.10340710
  description: |
    Cardiovascular diseases (CVDs) are the leading cause of death globally. Heart sound signal analysis plays an important role in clinical detection and physical examination of CVDs. In recent years, auxiliary diagnosis technology of CVDs based on the detection of heart sound signals has become a research hotspot. The detection of abnormal heart sounds can provide important clinical information to help doctors diagnose and treat heart disease. We propose a new set of fractal features-fractal dimension (FD)-as the representation for classification and a Support Vector Machine (SVM) as the classification model. The whole process of the method includes cutting heart sounds, feature extraction, and classification of abnormal heart sounds. We compare the classification results of the heart sound waveform (time domain) and the spectrum (frequency domain) based on fractal features. Finally, according to the better classification results, we choose the fractal features that are most conducive for classification to obtain better classification performance. The features we propose outperform the widely used features significantly (p < .05 by one-tailed z-test) with a much lower dimension. Clinical relevance-The heart sound classification model based on fractal provides a new time-frequency analysis method for heart sound signals. A new effective mechanism is proposed to explore the relationship between the heart sound acoustic properties and the pathology of CVDs. As a non-invasive diagnostic method, this work could supply an idea for the preliminary screening of cardiac abnormalities through heart sounds.
  image: images/EMBC_2023_zhu.jpg

- id: doi:10.1109/TMTT.2023.3308190
  description: |
    The challenge of noncontact presentation of human cardiopulmonary activity using a bioradar sensor is to linearly demodulate the Doppler cardiopulmonary diagram (DCD) signal from baseband signals. Arctangent demodulation can perform linear phase demodulation to obtain the DCD signal. However, the high-order harmonics and intermodulation terms (ITs) caused by the time-varying direct current (dc) offset and in-phase and quadrature-phase (I/Q) imbalance in the baseband signals significantly degrade the signal-to-noise ratio (SNR) of the Doppler heartbeat diagram (DHD) signal. In this work, a fast Fourier transform (FFT)-based algorithm is proposed to simultaneously perform time-varying dc offset compensation and I/Q imbalance correction without the need for an auxiliary device to improve the accuracy of the arctangent demodulation. The obtained results show that the SNRs of the algorithm-processed DHD signals are increased from 30.08 ± 2.41 to 68.88 ± 10.57 dB. In addition, the root mean square errors (RMSEs) of the C-C intervals of the DHD signals for eight subjects with respect to the J-J intervals of the ballistocardiogram (BCG) signals are 17.79 ± 2.72 ms (2.80% ± 0.43%), suggesting a promising potential of the DHD signal for noncontact biomedical applications.
  image: images/tian_1.jpg

- id: doi:10.1109/TBCAS.2023.3292237
  description: |
    For depression diagnosis, traditional methods such as interviews and clinical scales have been widely leveraged in the past few decades, but they are subjective, time-consuming, and labor-consuming. With the development of affective computing and Artificial Intelligence (AI) technologies, Electroencephalogram (EEG)-based depression detection methods have emerged. However, previous research has virtually neglected practical application scenarios, as most studies have focused on analyzing and modeling EEG data. Furthermore, EEG data is typically obtained from specialized devices that are large, complex to operate, and poorly ubiquitous. To address these challenges, a wearable three-lead EEG sensor with flexible electrodes was developed to obtain prefrontal-lobe EEG data. Experimental measurements show that the EEG sensor achieves promising performance (background noise of no more than 0.91 μVpp, Signal-to-Noise Ratio (SNR) of 26--48 dB, and electrode-skin contact impedance of less than 1 KΩ). In addition, EEG data from 70 depressed patients and 108 healthy controls were collected using the EEG sensor, and the linear and nonlinear features were extracted. The features were then weighted and selected using the Ant Lion Optimization (ALO) algorithm to improve classification performance. The experimental results show that the k-NN classifier achieves a classification accuracy of 90.70%, specificity of 96.53%, and sensitivity of 81.79%, indicating the promising potential of the three-lead EEG sensor combined with the ALO algorithm and the k-NN classifier for EEG-assisted depression diagnosis.
  image: images/tian_2.jpg

- id: doi:10.1109/TCSS.2024.3420445
  description: |
    Currently, the integration of artificial intelligence (AI) techniques with multimodal physiological signals represents a pivotal approach to detect affective disorders (ADs). With the increasing complexity and diversity of physiological signal modalities, researchers have introduced various AI methods using multimodal physiological signals to improve model classification performance and explainability to increase trust and facilitate clinical adoption. Among these methods, spiking neural networks (SNNs) stand out as a promising avenue due to their alignment with the operating principles of the human brain, robust biological explainability, and adeptness in processing spatial–temporal information in an efficient event-driven manner with low power consumption. Furthermore, the emergence of neuromorphic computing (NC) chips based on SNNs has greatly bolstered the field of NC, enabling effective support for objective, pervasive, and wearable AI-assisted medical diagnostic devices for ADs and other diseases. This article presents a review of recent achievements in multimodal AD detection and points out the associated challenges in utilizing multimodal physiological signals and NC based on SNNs for AD detection. Building upon this foundation, we give perspectives on future work. The intended readership for this review consists of researchers in the fields of cognitive computing, computational psychophysiology, affective computing, NC, and brain-inspired computing. We hope that this survey not only garners increased attention from the scientific community but also serves as a valuable guide for future studies in this field.
  image: images/tian_3.jpg

  
- id: doi:10.1109/BIBM58861.2023.10385577
  description: |
    Mental fatigue is a prevalent issue in contemporary society and can negatively affect physical performance and concentration, increasing the likelihood of adverse consequences due to inattention during productive activities. Therefore, it becomes increasingly important to address and eliminate fatigue within a specific period of time. Aromatherapy, as a form of Complementary Alternative Medicine (CAM), is a non-invasive, cost-effective, and efficient method to combat fatigue. Previous studies have assessed the effects of specific aromatherapy oils using scales, but there is a lack of objective and reliable physiological indicators to prove the effectiveness of aromatherapy. Hence, this paper seeks to establish a model illustrating the effects of aromatic essential oil gases on the human body. A multimodal physiological fatigue signal acquisition system that integrates aromatherapy feedback was designed. In addition, an experimental paradigm was developed to explore the potential of aromatherapy in mitigating mental fatigue. Electroencephalogram (EEG) and Electrocardiogram (ECG) signals were collected, allowing for the analysis of time-frequency domain features in EEG and ECG signals, as well as Heart Rate Variability (HRV) features in ECG signals. Our findings indicate that specific aromatic gases demonstrate effectiveness in reducing mental fatigue. Furthermore, we employed the Support Vector Machine (SVM) algorithm to classify the state of human mental fatigue. Based on the classification results, the release of aromatic gas was controlled to provide targeted aromatic feedback. This innovative approach offers a promising avenue for objectively assessing and addressing mental fatigue through aromatherapy interventions.
  image: images/tian_5.jpg

- id: 10.1109/TAFFC.2025.3557873
  description: |
   Mental disorders have increased rapidly and have emerged as a serious social health issue in the recent decade. Undoubtedly, the timely treatment of mental disorders is crucial. Emotion regulation has been proven to be an effective method for treating mental disorders. Music therapy as one of the methods that can achieve emotional regulation has gained increasing attention in the field of mental disorder treatment. However, traditional music therapy methods still face some unresolved issues, such as the lack of real-time capability and the inability to form closed-loop systems. With the advancement of artificial intelligence (AI), especially AI-generated content (AIGC), AI-based music therapy holds promise in addressing these issues. In this paper, an AIGC-based closed-loop music intervention system demonstration is proposed to regulate emotions for mental disorder treatment. This system demonstration consists of an emotion recognition model and a music generation model. The emotion recognition model can assess mental states, while the music generation model generates the corresponding emotional music for regulation. The system continuously performs recognition and regulation, thus forming a closed-loop process. In the experiment, we first conduct experiments on both the emotion recognition model and the music generation model to validate the accuracy of the recognition model and the music quality generated by the music generation models. In conclusion, we conducted comprehensive tests on the entire system to verify its feasibility and effectiveness.
  image: images/TAFFC_2025.jpg

