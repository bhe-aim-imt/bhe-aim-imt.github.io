- id: doi:10.1109/JBHI.2024.3487012
  description: |
    The development of affective computing and medical electronic technologies has led to the emergence of Artificial Intelligence (AI)-based methods for the early detection of depression. 
    However, previous studies have often overlooked the necessity for the AI-assisted diagnosis system to be wearable and accessible in practical scenarios for depression recognition. 
    In this work, we present an on-board executable multi-feature transfer-enhanced fusion model for our custom-designed wearable three-lead Electroencephalogram (EEG) sensor, based on EEG data collected from 73 depressed patients and 108 healthy controls. 
    Experimental results show that the proposed model exhibits low-computational complexity (65.0 K parameters), promising Floating-Point Operations (FLOPs) performance (25.6 M), real-time processing (1.5 s/execution), and low power consumption (320.8 mW). 
    Furthermore, it requires only 202.0 KB of Random Access Memory (RAM) and 279.6 KB of Read-Only Memory (ROM) when deployed on the EEG sensor. 
    Despite its low computational and spatial complexity, the model achieves a notable classification accuracy of 95.2%, specificity of 94.0%, and sensitivity of 96.9% under independent test conditions. 
    These results underscore the potential of deploying the model on the wearable three-lead EEG sensor for assisting in the diagnosis of depression.
  image: images/JBHI_2024.jpg

- id: doi:10.1109/TCE.2024.3514633
  description: |
    Mental disorders show a rapid increase and cause considerable harm to individuals as well as the society in recent decade. 
    Hence, mental disorders have become a serious public health challenge in nowadays society. 
    Timely treatment of mental disorders plays a critical role for reducing the harm of mental illness to individuals and society. 
    Music therapy is a type of non-pharmaceutical method in treating such mental disorders. 
    However, conventional music therapy suffers from a number of issues resulting in a lack of popularity. 
    Thanks to the rapid development of Artificial Intelligence (AI), especially the AI Generated Content (AIGC), it provides a chance to address these issues. Nevertheless, to the best of our knowledge, there is no work investigating music therapy from AIGC and closed-loop perspective. 
    In this paper, we summarise some universal music therapy methods and discuss their shortages. Then, we indicate some AIGC techniques, especially the music generation, for their application in music therapy. 
    Moreover, we present a closed-loop music therapy system and introduce its implementation details. 
    Finally, we discuss some challenges in AIGC-based music therapy with proposing further research direction, and we suggest the potential of this system to become a consumer-grade product for treating mental disorders.
  image: images/TCE_2024.jpg
  date: 2024-12-01

- id: doi:10.1109/TCSS.2023.3235079
  description: |
    Welcome to the first issue of IEEE Transactions on Computational Social Systems (TCSS) of 2023. 
    The past 2022 was again a very productive year, in which we have published 159 articles with about 1850 pages in six issues. 
    We also received much great and exciting news.
  image: images/TCSS_2023.jpg

- id: doi:10.1109/GCCE59613.2023.10315503
  description: |
    Music generation with artificial intelligence is a complex and captivating task. 
    The utilisation of generative adversarial networks (GANs) has exhibited promising outcomes in producing realistic and diverse music compositions. 
    In this paper, we propose a model based on Wasserstein GAN with gradient penalty (WGAN-GP) for multi-track music generation. 
    This model incorporates self-attention and introduces a novel cross-attention mechanism in the generator to enhance its expressive capability. 
    Additionally, we transpose all music to C major in training to ensure data consistency and quality. 
    Experimental results demonstrate that our model can produce multi-track music with enhanced rhythm and sound characteristics, accelerate convergence, and improve generation quality.
  image: images/GCCE_2023.jpg

- id: doi:10.21437/Interspeech.2024-685
  description: |
    Recognising the widest range of emotions possible is a major challenge in the task of Speech Emotion Recognition(SER), especially for complex and mixed emotions. 
    However, due to the limited number of emotional types and uneven distribution of data within existing datasets, current SER models are typically trained and used in a narrow range of emotional types. 
    In this paper, we propose the Emotion Open Deep Network(E-ODN) model to address this issue. 
    Besides, we introduce a novel Open-Set Recognition method that maps sample emotional features into a three-dimensional emotional space. 
    The method can infer unknown emotions and initialise new type weights, enabling the model to dynamically learn and infer emerging emotional types. 
    The empirical results show that our recognition model outperforms the state-of-the-art(SOTA) models in dealing with multi-type unbalanced data, and it can also perform finer-grained emotion recognition.
  image: images/ISP_2024.jpg

- id: doi:10.1109/TMC.2025.3547842
